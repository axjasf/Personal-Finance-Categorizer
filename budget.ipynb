{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1gyo8C4G_Qtqp_YTsSNLELTOoaZDs0SF2",
      "authorship_tag": "ABX9TyOXg8lDTqlvXIVRILWrsyem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/bugfixes/budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Setup"
      ],
      "metadata": {
        "id": "jDWIT_CWmLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paths"
      ],
      "metadata": {
        "id": "l1dgHOE5Tjap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3u8iiJThN_pc"
      },
      "outputs": [],
      "source": [
        "# Path settings\n",
        "HOME_PATH = \"/content/drive/MyDrive/Colab Notebooks/budget/\"\n",
        "CONFIG_PATH = HOME_PATH + \"config/\"\n",
        "TRANSACTIONS_PATH = HOME_PATH + \"transactions/\"\n",
        "ORDERS_PATH = HOME_PATH + \"orders/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of Libraries"
      ],
      "metadata": {
        "id": "YFUcNTnFoxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "754FWtrPPMRN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define global Variables"
      ],
      "metadata": {
        "id": "MQ1-J3IcotCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transactions dataframe and load the JSON configuration for the different banks\n",
        "bank_transactions = {}\n",
        "\n",
        "transactions_file = \"transactions.csv\"\n",
        "\n",
        "config_files = {\n",
        "    \"Payee Matching\" : \"payee_matching.json\",\n",
        "    \"Split Distribution\" : \"payee_category_split.csv\",\n",
        "    \"Exchange Rates EUR USD\": 'eur_usd_exchange_rates.csv',\n",
        "    \"Amazon Item Categories\": 'amazon_item_categories.csv'\n",
        "}\n",
        "\n",
        "# If we are using Google Drive, prefix each value in the dictionary with the ..._PATH variable\n",
        "config_files = {key: f\"{CONFIG_PATH}{value}\" for key, value in config_files.items()}"
      ],
      "metadata": {
        "id": "8W7j8YnVNfEX"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Payees"
      ],
      "metadata": {
        "id": "qvAQCjR02_0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Read Transaction files"
      ],
      "metadata": {
        "id": "SDE5kW_isiMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset variables before reading the file\n",
        "all_transactions = []\n",
        "\n",
        "# Define overall transactions dataframe\n",
        "all_transactions = pd.read_csv(TRANSACTIONS_PATH + transactions_file)\n",
        "\n",
        "# Convert 'Amount (USD)' to float\n",
        "all_transactions['Amount (USD)'] = pd.to_numeric(all_transactions['Amount (USD)'], errors='coerce')\n",
        "\n",
        "# Convert the date columns to consistent datetime format\n",
        "all_transactions['Date'] = pd.to_datetime(all_transactions['Date'])\n",
        "\n",
        "print(len(all_transactions))"
      ],
      "metadata": {
        "id": "4xnCKucEslAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b692f5-3d9b-49bd-e055-a8cd9d897d07"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Payee mapping and Category assignment"
      ],
      "metadata": {
        "id": "IQ2XK_tVq09c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "class MerchantMatcher:\n",
        "    def __init__(self, data_df):\n",
        "        self.data = data_df\n",
        "        self.vectorizer = self._train_vectorizer()\n",
        "        self.payee_vectors = self._compute_payee_vectors()\n",
        "        self.positive_list_descriptions = self._get_positive_list_descriptions()\n",
        "\n",
        "    def _match_prefix(self, description, merchant_details):\n",
        "        prefix_length = merchant_details.get('PrefixLength', 50)\n",
        "        for category in merchant_details['Categories']:\n",
        "            for known_description in category['Descriptions']:\n",
        "                truncated_payee = known_description.lower()[:prefix_length]\n",
        "                if description.lower().startswith(truncated_payee):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _train_vectorizer(self):\n",
        "        all_descriptions = [desc.lower() for category_list in self.data['Categories'] for category in category_list for desc in category['Descriptions']]\n",
        "        return TfidfVectorizer().fit(all_descriptions)\n",
        "\n",
        "    def _compute_payee_vectors(self):\n",
        "        payee_vectors = {}\n",
        "        for merchant, details in self.data.iterrows():\n",
        "            descriptions = [desc.lower() for category in details['Categories'] for desc in category['Descriptions']]\n",
        "            if not descriptions:\n",
        "                continue\n",
        "            tfidf_matrix = self.vectorizer.transform(descriptions)\n",
        "            avg_vector = np.asarray(tfidf_matrix.mean(axis=0))\n",
        "            payee_vectors[merchant] = avg_vector\n",
        "        return payee_vectors\n",
        "\n",
        "    def _get_positive_list_descriptions(self):\n",
        "        return set(desc.lower() for category_list in self.data['Categories'] for category in category_list for desc in category['Descriptions'])\n",
        "\n",
        "    def predict_payees(self, transaction_df):\n",
        "        mg_values = []\n",
        "        chkpayee_values = []\n",
        "        category_types = []\n",
        "        categories = []\n",
        "        candidates = []\n",
        "\n",
        "        for _, row in transaction_df.iterrows():\n",
        "            description_lower = row['Description'].lower() if row['Description'] else None\n",
        "            current_merchant = None\n",
        "            current_chkpayee = None\n",
        "            current_category_type = None\n",
        "            current_category = None\n",
        "            max_match_length = 0  # To keep track of the length of the matching description\n",
        "\n",
        "            if pd.isna(description_lower) or not description_lower.strip():\n",
        "                mg_values.append(None)\n",
        "                chkpayee_values.append(None)\n",
        "                category_types.append(None)\n",
        "                categories.append(None)\n",
        "                continue\n",
        "\n",
        "            for merchant, details in self.data.iterrows():\n",
        "                for category in details.get('Categories', []):\n",
        "                    for desc in category.get('Descriptions', []):\n",
        "                        if desc.lower() in description_lower and len(desc) > max_match_length:\n",
        "                            current_merchant = merchant\n",
        "                            current_chkpayee = 'A'\n",
        "                            current_category_type = category.get('Category Type')\n",
        "                            current_category = category.get('Category')\n",
        "                            max_match_length = len(desc)  # Update the max_match_length\n",
        "                            break\n",
        "\n",
        "                    # Check for prefix matching if no match is found yet\n",
        "                    if not current_merchant and self._match_prefix(description_lower, details):\n",
        "                        current_merchant = merchant\n",
        "                        current_chkpayee = 'P'\n",
        "                        current_category_type = category.get('Category Type')\n",
        "                        current_category = category.get('Category')\n",
        "                        break\n",
        "\n",
        "                if current_merchant:  # Break the outer loop if a match is found\n",
        "                    break\n",
        "\n",
        "            if not current_merchant:\n",
        "                description_vector = self.vectorizer.transform([description_lower])\n",
        "                similarities = {merchant: linear_kernel(description_vector, np.asarray(vector))[0][0] for merchant, vector in self.payee_vectors.items()}\n",
        "                predicted_merchant = max(similarities, key=similarities.get)\n",
        "                max_similarity = similarities[predicted_merchant]\n",
        "\n",
        "                if max_similarity > self.data.loc[predicted_merchant, 'Threshold']:\n",
        "                    candidates.append({'Payee': predicted_merchant, 'Description': row['Description'], 'Probability': max_similarity})\n",
        "\n",
        "            mg_values.append(current_merchant)\n",
        "            chkpayee_values.append(current_chkpayee or 'C')\n",
        "            category_types.append(current_category_type)\n",
        "            categories.append(current_category)\n",
        "\n",
        "        transaction_df['Payee'] = mg_values\n",
        "        transaction_df['chkPayee'] = chkpayee_values\n",
        "        transaction_df['Category Type'] = category_types\n",
        "        transaction_df['Category'] = categories\n",
        "        candidates_df = pd.DataFrame(candidates)\n",
        "        return transaction_df, candidates_df\n",
        "\n",
        "data_df = pd.read_json(config_files[\"Payee Matching\"], orient=\"index\")\n",
        "\n",
        "matcher = MerchantMatcher(data_df)\n",
        "payees_identified_df, payees_candidates_df = matcher.predict_payees(all_transactions)\n",
        "payees_identified_df = payees_identified_df[payees_identified_df['chkPayee'] != 'C']\n",
        "\n",
        "file_payees_identified = \"1_2_chk_payees_identified.csv\"\n",
        "file_payees_candidates = \"1_2_chk_payees_candidates.csv\"\n",
        "\n",
        "if os.path.exists(file_payees_identified): os.remove(file_payees_identified)\n",
        "if os.path.exists(file_payees_candidates): os.remove(file_payees_candidates)\n",
        "payees_identified_df.to_csv(file_payees_identified, index=False)\n",
        "payees_candidates_df.to_csv(file_payees_candidates, index=False)\n",
        "\n",
        "print(len(all_transactions)) #Achtung: M und S Transaktionen, Splits!\n",
        "\n",
        "all_transactions.to_csv(\"1_2_df_all_payee_mapping_done.csv\", index=False)"
      ],
      "metadata": {
        "id": "qh_Ltgm3iSlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb9a961-c154-4877-b569-daeecff770df"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Categories"
      ],
      "metadata": {
        "id": "DF0bxKmWmsz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Split assignment"
      ],
      "metadata": {
        "id": "u4CttrmftPPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitProcessor:\n",
        "    def __init__(self, all_transactions, split_data):\n",
        "        self.all_transactions = all_transactions\n",
        "        self.split_data = split_data\n",
        "        self.split_transactions = []\n",
        "\n",
        "    def _mark_as_master(self, idx):\n",
        "        self.all_transactions.at[idx, 'Category Type'] = ''\n",
        "        self.all_transactions.at[idx, 'Category'] = ''\n",
        "        self.all_transactions.at[idx, 'SplitID'] = str(self.all_transactions.at[idx, 'Account-ID']) + '-' + 'M'\n",
        "        self.all_transactions.at[idx, 'chkSplit'] = 'M'\n",
        "        self.all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "    def _add_split_rows(self, row, payee):\n",
        "        split_details = self.split_data[self.split_data['Payee'] == payee]\n",
        "        for _, category in split_details.iterrows():\n",
        "            new_row = row.copy()\n",
        "            new_row['Category Type'] = category['Category Type']\n",
        "            new_row['Category'] = category['Category']\n",
        "            new_row['SplitID'] = str(row['Account-ID']) + '-' + 'S'\n",
        "            new_row['chkSplit'] = 'S'\n",
        "            new_row['chkCategory'] = 'A'\n",
        "            new_row['Amount (USD)'] = row['Amount (USD)'] * category['Percentage']\n",
        "            self.split_transactions.append(new_row)\n",
        "\n",
        "    def process_splits(self):\n",
        "        for idx, row in self.all_transactions.iterrows():\n",
        "            payee = row['Payee']\n",
        "            split_details = self.split_data[self.split_data['Payee'] == payee]\n",
        "            if not split_details.empty:\n",
        "                self._mark_as_master(idx)  # Mark the original row in all_transactions as Master\n",
        "                self._add_split_rows(row, payee)\n",
        "        # Add the split rows to the all_transactions dataframe\n",
        "        self.all_transactions = pd.concat([self.all_transactions, pd.DataFrame(self.split_transactions)], ignore_index=True)\n",
        "        return len(self.all_transactions)\n",
        "\n",
        "    def get_updated_transactions(self):\n",
        "        return self.all_transactions\n",
        "\n",
        "split_data = pd.read_csv(config_files['Split Distribution'])\n",
        "processor = SplitProcessor(all_transactions, split_data)\n",
        "processor.process_splits()\n",
        "all_transactions = processor.get_updated_transactions()\n",
        "\n",
        "all_transactions.to_csv(\"2_1_df_all_split_done.csv\", index=False)"
      ],
      "metadata": {
        "id": "s4LpNZWV_dgB"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Amazon Order Matching"
      ],
      "metadata": {
        "id": "VO10hFL8P3GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Header to Transaction Matching"
      ],
      "metadata": {
        "id": "pxxoICcdP8n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_order_headers = pd.read_csv(ORDERS_PATH + \"amazon_headers.csv\", parse_dates=['Order Date', 'Payment Date'])"
      ],
      "metadata": {
        "id": "G05g5zt07HT0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonDataMatcher:\n",
        "    def __init__(self, headers_df, transactions_df):\n",
        "        self.headers_df = headers_df\n",
        "        self.transactions_df = transactions_df\n",
        "        self.matched_df = None\n",
        "\n",
        "    def match_records(self):\n",
        "        # Filter transactions based on Payee criteria\n",
        "        valid_payees = [\"Amazon\", \"Amazon Grocery\", \"Amazon Prime Video\"]\n",
        "        filtered_transactions = self.transactions_df[self.transactions_df['Payee'].isin(valid_payees)]\n",
        "\n",
        "        # Create an empty dataframe to store the matched records\n",
        "        matched_records = pd.DataFrame()\n",
        "\n",
        "        # Iterate over each row in the Amazon order headers\n",
        "        for idx, row in self.headers_df.iterrows():\n",
        "            # Filter for transactions within a 5-day range\n",
        "            date_range_mask = (filtered_transactions['Date'] >= row['Payment Date'] - pd.Timedelta(days=5)) & \\\n",
        "                              (filtered_transactions['Date'] <= row['Payment Date'] + pd.Timedelta(days=5))\n",
        "\n",
        "            # Filter for matching payment amount\n",
        "            amount_mask = filtered_transactions['Amount (USD)'] == -row['Payment Amount']\n",
        "\n",
        "            # Get the matching transaction if it exists\n",
        "            matching_transaction = filtered_transactions[date_range_mask & amount_mask]\n",
        "\n",
        "            # If a matching transaction is found, concatenate to the matched_records dataframe\n",
        "            if not matching_transaction.empty:\n",
        "                matched_row = pd.concat([row, matching_transaction.iloc[0]])\n",
        "                matched_records = pd.concat([matched_records, matched_row.to_frame().T], ignore_index=True)\n",
        "\n",
        "        self.matched_df = matched_records\n",
        "\n",
        "    def save_to_csv(self, output_path):\n",
        "        if self.matched_df is not None:\n",
        "            self.matched_df.to_csv(output_path, index=False)\n",
        "            print(f\"Matching complete and saved to {output_path}!\")\n",
        "        else:\n",
        "            print(\"No matched data to save.\")\n",
        "\n",
        "    def verify_match(self):\n",
        "        # Verification on the Amount (USD) level\n",
        "        matched_amounts_sum = self.matched_df['Payment Amount'].sum()\n",
        "        transactions_amounts_sum = self.transactions_df['Amount (USD)'].sum()\n",
        "\n",
        "        # Check if the sums are approximately equal, considering potential rounding errors\n",
        "        if abs(matched_amounts_sum + transactions_amounts_sum) < 0.05:\n",
        "            print(\"Verification passed: Matched amounts are consistent with the transaction amounts.\")\n",
        "        else:\n",
        "            print(\"Verification failed: There's a discrepancy in the matched amounts.\")\n",
        "\n",
        "    def update_transactions_with_matches(self, all_transactions):\n",
        "        # Left merge to get matching 'Order ID' into the all_transactions DataFrame\n",
        "        merged = all_transactions.merge(self.matched_df[['Account-ID', 'Order ID']],\n",
        "                                        on='Account-ID',\n",
        "                                        how='left')\n",
        "\n",
        "        # Update the 'Category' column with 'Order ID' where matches occurred\n",
        "        merged['Category'] = merged['Order ID'].combine_first(merged['Category'])\n",
        "\n",
        "        # Drop the 'Order ID' column as it's no longer needed\n",
        "        merged.drop('Order ID', axis=1, inplace=True)\n",
        "\n",
        "        return merged\n",
        "\n",
        "# Create an instance of the class\n",
        "matcher = AmazonDataMatcher(amazon_order_headers, all_transactions)\n",
        "\n",
        "# Match the records\n",
        "matcher.match_records()\n",
        "\n",
        "# Verify the match\n",
        "matcher.verify_match()\n",
        "\n",
        "# Save the matched data (optional)\n",
        "matcher.save_to_csv('2_2_1_amazon_order_matched_records.csv')\n",
        "\n",
        "matched_transactions_with_headers = matcher.matched_df\n",
        "\n",
        "all_transactions.to_csv(\"2_2_1_df_all_header_to_txn_matching_done.csv\", index=False)"
      ],
      "metadata": {
        "id": "f7Oz_2cRg-WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1b7a43-aed6-4352-d58c-f6f462b789a2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification failed: There's a discrepancy in the matched amounts.\n",
            "Matching complete and saved to 2_2_1_amazon_order_matched_records.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Item to Transaction Categorizer"
      ],
      "metadata": {
        "id": "8Fv0aBdc37Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.2.2.1 Item to Transaction mapping\n",
        "\n",
        "amazon_order_items = pd.read_csv(ORDERS_PATH + \"amazon_items.csv\", parse_dates=['Order Date'])\n",
        "keyword_df = pd.read_csv(config_files['Amazon Item Categories'])\n",
        "\n",
        "# Merge the dataframes\n",
        "final_df = pd.merge(matched_transactions_with_headers, amazon_order_items[['Order ID', 'Item Description', 'Item Category', 'Price', 'Quantity', 'chkMatch']], on='Order ID', how=\"left\")\n",
        "\n",
        "final_df = final_df[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "#    'SplitID',\n",
        "#    'Payee',\n",
        "#    'Category Type',\n",
        "#    'Category',\n",
        "#    'Amount (USD)',\n",
        "    'Order ID',\n",
        "    'Payment Date',\n",
        "    'Payment Amount',\n",
        "#    'Category Type',\n",
        "#    'Category',\n",
        "#    'Description',\n",
        "    'Item Description',\n",
        "    'Item Category',\n",
        "    'chkMatch',\n",
        "    'Price',\n",
        "    'Quantity',\n",
        "    'Total',\n",
        "    'Shipping',\n",
        "    'Shipping Refund',\n",
        "    'Gift',\n",
        "    'Tax',\n",
        "    'Refund',\n",
        "    'Origin',\n",
        "#    'Currency',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "#    'chkEURUSD'\n",
        "    ]]\n",
        "\n",
        "final_df.to_csv(\"2_2_2_1_df_item_to_txn_mapping_done.csv\", index=False)"
      ],
      "metadata": {
        "id": "upsTJv6_QCeA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.2.2.2 - Transaction splitting and amount reconciliation by category group and item\n",
        "\n",
        "class AmazonProcessorDebugStep2V5Revised:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df.sort_values(by='Order ID')\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def modify_all_transactions_step2(self):\n",
        "        modified_amazon_transactions = []\n",
        "\n",
        "        # Process Amazon Transactions by grouping by Account-ID\n",
        "        for account_id, account_group in self.final_df.groupby(['Account-ID']):\n",
        "            total_summe = (account_group['Price'] * account_group['Quantity']).sum()\n",
        "            actual_payment = self.all_transactions[self.all_transactions['Account-ID'] == account_id]['Amount (USD)'].iloc[0]\n",
        "\n",
        "            # For single record\n",
        "            if len(account_group) == 1:\n",
        "                master_row = self.all_transactions[self.all_transactions['Account-ID'] == account_id].iloc[0].copy()\n",
        "                master_row['Category'] = account_group['Item Category'].iloc[0]\n",
        "                master_row['chkSplit'] = \"U\"\n",
        "                master_row['SplitID'] = master_row['Account-ID'] + \"-U\"\n",
        "                master_row['Item Description'] = account_group['Item Description'].iloc[0]  # This line is the adjustment\n",
        "                modified_amazon_transactions.append(master_row.to_dict())\n",
        "            else:\n",
        "                # Master record (only if there are splits)\n",
        "                master_row = self.all_transactions[self.all_transactions['Account-ID'] == account_id].iloc[0].copy()\n",
        "                master_row['chkSplit'] = \"M\"\n",
        "                master_row['SplitID'] = master_row['Account-ID'] + \"-M\"\n",
        "                master_row['Category'] = np.nan  # Removing category for master record with splits\n",
        "                modified_amazon_transactions.append(master_row.to_dict())\n",
        "\n",
        "                # Splits for categorized items\n",
        "                split_counter = 0\n",
        "                for _, category_group in account_group.groupby('Item Category'):\n",
        "                    category_summe = (category_group['Price'] * category_group['Quantity']).sum()\n",
        "                    category_percentage = category_summe / total_summe\n",
        "                    adjusted_amount = actual_payment * category_percentage\n",
        "\n",
        "                    # Create a new row for the modified transaction\n",
        "                    split_row = category_group.iloc[0].copy()\n",
        "                    split_row['Payee'] = 'Amazon'\n",
        "                    split_row['SplitID'] = split_row['Account-ID'] + \"-S\" + str(split_counter)\n",
        "                    split_row['chkSplit'] = \"S\"\n",
        "                    split_row['Category'] = category_group['Item Category'].iloc[0]\n",
        "                    split_row['Amount (USD)'] = adjusted_amount\n",
        "                    split_row['Item Description'] = np.nan\n",
        "                    modified_amazon_transactions.append(split_row.to_dict())\n",
        "\n",
        "                    split_counter += 1\n",
        "\n",
        "                # Additional Splits for uncategorized items\n",
        "                uncategorized_items = account_group[account_group['Item Category'].isna()]\n",
        "                for _, row in uncategorized_items.iterrows():\n",
        "                    uncategorized_split = row.copy()\n",
        "                    uncategorized_split['Payee'] = 'Amazon'\n",
        "                    uncategorized_split['SplitID'] = uncategorized_split['Account-ID'] + \"-S\" + str(split_counter)\n",
        "                    uncategorized_split['chkSplit'] = \"S\"\n",
        "                    uncategorized_split['Category'] = np.nan\n",
        "\n",
        "                    # Adjusting the amount for uncategorized splits\n",
        "                    uncategorized_summe = uncategorized_split['Price'] * uncategorized_split['Quantity']\n",
        "                    uncategorized_percentage = uncategorized_summe / total_summe\n",
        "                    uncategorized_adjusted_amount = actual_payment * uncategorized_percentage\n",
        "                    uncategorized_split['Amount (USD)'] = uncategorized_adjusted_amount\n",
        "\n",
        "                    modified_amazon_transactions.append(uncategorized_split.to_dict())\n",
        "\n",
        "                    split_counter += 1\n",
        "\n",
        "        # Combine the modified Amazon transactions with the all_transactions dataframe\n",
        "        modified_amazon_df = pd.DataFrame(modified_amazon_transactions)\n",
        "        # Remove original Amazon records\n",
        "        self.all_transactions = self.all_transactions[~self.all_transactions['Account-ID'].isin(modified_amazon_df['Account-ID'])]\n",
        "        # Combine with modified Amazon transactions\n",
        "        combined_df = pd.concat([self.all_transactions, modified_amazon_df], ignore_index=True)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "\n",
        "# Apply the method\n",
        "processor_debug_step2_v5_revised = AmazonProcessorDebugStep2V5Revised(final_df, all_transactions)\n",
        "combined_transactions_step2_v5_revised = processor_debug_step2_v5_revised.modify_all_transactions_step2()\n",
        "\n",
        "# Output to CSV\n",
        "combined_transactions_step2_v5_revised.to_csv(\"2_2_2_2_txn_splitting_done.csv\", index=False)\n",
        "\n",
        "all_transactions = combined_transactions_step2_v5_revised"
      ],
      "metadata": {
        "id": "RsL1CSGcqKG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a817ac4c-a77b-4984-abb6-122a12412b43"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-c10ee8563197>:12: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
            "  for account_id, account_group in self.final_df.groupby(['Account-ID']):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Output"
      ],
      "metadata": {
        "id": "auc-YLI0qPmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Dataframe preparation"
      ],
      "metadata": {
        "id": "Ojxb0cORqS37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder Columns\n",
        "all_transactions = all_transactions[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "#    'Order ID', #just for testing issue #16\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Item Description',\n",
        "    'Description',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkSplit',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "# Sort rows\n",
        "all_transactions = all_transactions.sort_values(by=['Date', 'Account-ID', 'SplitID'], ascending=[False, True, True])\n",
        "\n",
        "# Formating\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].round(2)\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].apply(lambda x: \"${:,.2f}\".format(x))\n"
      ],
      "metadata": {
        "id": "EJTopk0eo8Ec"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Output file generation"
      ],
      "metadata": {
        "id": "tbGNVP1OrelH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"z_output.csv\"): os.remove(\"z_output.csv\")\n",
        "all_transactions.to_csv(\"z_output.csv\", index=False)"
      ],
      "metadata": {
        "id": "glF2a7uTBl9T"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}