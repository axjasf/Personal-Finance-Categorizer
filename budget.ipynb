{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXxJjQAdWc2Y2oNi6KVgVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "* This project is meant to bring all my personal finance related transactions into one easy to understand view.\n",
        "* Scope / Value descriptoon\n",
        "    * ...\n",
        "* Mechanism\n",
        "    * It reads CSV files from several US and German banks and Credit Card processors and harmonizes them into one dataframe.\n",
        "    * It maps fields such as descriptions into payees\n",
        "        * Lookup mechanism (positive and negative lists) against a payee config JSON file\n",
        "        * Fuzzy matching against pre-determined patterns\n",
        "    * It categorizes each transaction or splits it into several categories\n",
        "        * by payee\n",
        "        * by pre-determination of a percentage split (e.g. for Walgreens that should be sufficient, given that I have categorized transactions since 2014)\n",
        "        * by semi-automatic order-item review split (e.g. for Apple or Amazon transactions where these files exist and where a split between utility and subscription or grocery, household products or general shopping is of interest)\n",
        "    * It works with a set of indicator field to mark aspects of interest\n",
        "        * Indicator for transactions in which automatic determinations have been taken place\n",
        "        * Task field to address open tasks\n",
        "        * ..."
      ],
      "metadata": {
        "id": "jp_J4Oz5euR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jDWIT_CWmLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Libraries\n",
        "\n",
        "*   Neccessary libraries that might not be available right away in CoLab need to be installed here.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EvOqV2xgovT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of Libraries\n",
        "* Loading of neccessary libraries such as Pandas etc."
      ],
      "metadata": {
        "id": "YFUcNTnFoxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "754FWtrPPMRN"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define global Variables\n",
        "* Create transactions structure that ultimately will hold the transactions dataframes from all bank files\n",
        "* Create overall transactions dataframe"
      ],
      "metadata": {
        "id": "MQ1-J3IcotCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transactions dataframe and load the JSON configuration for the different banks\n",
        "bank_transactions = {}\n",
        "\n",
        "bank_files = {\n",
        "        \"Chase\": \"chase.csv\",\n",
        "        \"Wells Fargo Checking\": \"wells_fargo_checking.csv\",\n",
        "        \"Apple\": \"apple.csv\",\n",
        "        \"Commerzbank\": \"commerzbank.csv\"\n",
        "    }"
      ],
      "metadata": {
        "id": "8W7j8YnVNfEX"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Conversion"
      ],
      "metadata": {
        "id": "DonELYoNmHRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For each bank file:\n",
        "    * Load file into individual df\n",
        "    * Basic quality control on the individual df level\n",
        "    * Transform columns into target columns\n",
        "        * Add Bank ID field as well as numberical ID field\n",
        "    * Add individual df to transactions df\n",
        "\n",
        "* Special transformations for non-US banks:\n",
        "    * Date conversion\n",
        "    * EUR to USD conversion based on an existing file (date and exchange rate or an API call to a free service)"
      ],
      "metadata": {
        "id": "sV_EgVu5pG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_control(df):\n",
        "    missing_values = df.isnull().sum()\n",
        "    column_data_types = df.dtypes\n",
        "\n",
        "    return missing_values, column_data_types"
      ],
      "metadata": {
        "id": "we4_xBVmKZPX"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_field_names(df, bank=\"\"):\n",
        "    #rename original columns to \"org_\" + fieldname\n",
        "    #df.columns = ['org_' + col for col in df.columns]\n",
        "\n",
        "    if bank == \"Chase\":\n",
        "        df.rename(columns={\"Category\" : \"oldCategory\"})\n",
        "\n",
        "    df.insert(0, 'Account',bank)\n",
        "    df.insert(1, 'ID', range(1, len(df) + 1))\n",
        "    df.insert(2, 'SubID',\"\")\n",
        "    df.insert(3, 'Date','')\n",
        "    df.insert(4, 'Payee','')\n",
        "    df.insert(5, 'Category Type','')\n",
        "    df.insert(6, 'Category','')\n",
        "\n",
        "    if bank == \"Commerzbank\":\n",
        "        df.insert(\"Amount (USD)\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sx688qkjJyLA"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Chase'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]"
      ],
      "metadata": {
        "id": "3ayfeFcIJeLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b36d26c7-c9d6-460a-cf26-9c1bbb57ad45"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-65f2ed03c4af>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbank_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbank_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_field_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbank_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbank_transactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-68a0e4b5de80>\u001b[0m in \u001b[0;36madjust_field_names\u001b[0;34m(df, bank)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Payee'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Category Type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Commerzbank\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4817\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4819\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot insert Category, already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Apple'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = add_new_fields(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])"
      ],
      "metadata": {
        "id": "Fb8FLo8XeVqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Commerzbank'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = add_new_fields(bank_transactions[bank])\n",
        "\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction date'], errors='coerce', format='%d.%m.%Y') # For Commerzbank, Day.Month.Year\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "### https://www.wsj.com/market-data/quotes/fx/EURUSD/historical-prices\n",
        "\n",
        "exchange_rates_data = pd.read_csv('eur_usd_exchange_rates.csv')\n",
        "\n",
        "# Convert the date columns to consistent datetime format\n",
        "exchange_rates_data['Date'] = pd.to_datetime(exchange_rates_data['Date'], format='%m/%d/%Y')\n",
        "\n",
        "# Merge on the date columns to add the exchange rate to bank_transactions[bank]\n",
        "bank_transactions[bank] = bank_transactions[bank].merge(exchange_rates_data[['Date', ' Close']], on='Date', how='left')\n",
        "\n",
        "# Convert the Amount from EUR to USD\n",
        "bank_transactions[bank]['Amount_in_USD'] = bank_transactions[bank]['Amount'] * bank_transactions[bank][' Close']\n",
        "\n",
        "# Drop the ' Close' column as it's not needed anymore in bank_transactions[bank]\n",
        "bank_transactions[bank].drop(' Close', axis=1, inplace=True)\n",
        "\n",
        "# Display the result with the new Amount_in_USD column\n",
        "bank_transactions[bank][['Date', 'Amount', 'Amount_in_USD']].head()\n",
        "\n",
        "\n",
        "\n",
        "bank_transactions[bank]"
      ],
      "metadata": {
        "id": "qLtMUpw7e1Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harmonization"
      ],
      "metadata": {
        "id": "IQ2XK_tVq09c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* New Payee identification\n",
        "    * Match payee against in a config file for payees\n",
        "* Payee transformation"
      ],
      "metadata": {
        "id": "h3dHnqhpq5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "class MerchantMatcher:\n",
        "    def __init__(self, data_df):\n",
        "        self.data = data_df\n",
        "        self.vectorizer = self._train_vectorizer()\n",
        "        self.payee_vectors = self._compute_payee_vectors()\n",
        "        self.positive_list_descriptions = self._get_positive_list_descriptions()\n",
        "\n",
        "    def _train_vectorizer(self):\n",
        "        all_descriptions = [desc for descriptions in self.data['Positive List'] for desc in descriptions]\n",
        "        return TfidfVectorizer().fit(all_descriptions)\n",
        "\n",
        "    def _compute_payee_vectors(self):\n",
        "        payee_vectors = {}\n",
        "        for merchant, details in self.data.iterrows():\n",
        "            tfidf_matrix = self.vectorizer.transform(details['Positive List'])\n",
        "            avg_vector = np.asarray(tfidf_matrix.mean(axis=0))\n",
        "            payee_vectors[merchant] = avg_vector\n",
        "        return payee_vectors\n",
        "\n",
        "    def _get_positive_list_descriptions(self):\n",
        "        return set(desc for descriptions in self.data['Positive List'] for desc in descriptions)\n",
        "\n",
        "    def predict_payees(self, transaction_df):\n",
        "        mg_values = []\n",
        "        candidates = []\n",
        "\n",
        "        for _, row in transaction_df.iterrows():\n",
        "            description = row['Description']\n",
        "\n",
        "            if pd.isna(description) or not description.strip():\n",
        "                mg_values.append(None)\n",
        "                continue\n",
        "\n",
        "            if description in self.positive_list_descriptions:\n",
        "                for merchant, details in self.data.iterrows():\n",
        "                    if description in details['Positive List']:\n",
        "                        mg_values.append(merchant)\n",
        "                        break\n",
        "            else:\n",
        "                description_vector = self.vectorizer.transform([description])\n",
        "                similarities = {merchant: linear_kernel(description_vector, np.asarray(vector))[0][0] for merchant, vector in self.payee_vectors.items()}\n",
        "                predicted_merchant = max(similarities, key=similarities.get)\n",
        "                max_similarity = similarities[predicted_merchant]\n",
        "\n",
        "                if max_similarity > self.data.loc[predicted_merchant, 'Threshold']:\n",
        "                    mg_values.append(predicted_merchant)\n",
        "                    candidates.append({'Payee': predicted_merchant, 'Description': description, 'Probability': max_similarity})\n",
        "                else:\n",
        "                    mg_values.append(None)\n",
        "\n",
        "        transaction_df['Merchant'] = mg_values\n",
        "        candidates_df = pd.DataFrame(candidates)\n",
        "        return transaction_df, candidates_df\n",
        "\n",
        "# Sample Usage\n",
        "data_df = pd.read_json(\"payee_matching.json\", orient=\"index\")  # Replace with your DataFrame loading mechanism\n",
        "transactions_df = bank_transactions['Chase']  # Replace with your transaction DataFrame\n",
        "\n",
        "matcher = MerchantMatcher(data_df)\n",
        "predicted_df, candidates_df = matcher.predict_payees(transactions_df)\n",
        "\n",
        "if os.path.exists(\"merchant_guess.csv\"): os.remove(\"merchant_guess.csv\")\n",
        "if os.path.exists(\"candidates.csv\"): os.remove(\"candidates.csv\")\n",
        "predicted_df.to_csv(\"merchant_guess.csv\", index=False)\n",
        "candidates_df.to_csv(\"candidates.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_Ltgm3iSlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorization"
      ],
      "metadata": {
        "id": "DF0bxKmWmsz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transactions <--> Payee mapping (1:1)\n",
        "* Transactions <--> Amazon Orders mapping and splitting\n",
        "* Transactions <--> Apple Orders mapping and splitting\n",
        "* Transactions <--> Walgreens splitting"
      ],
      "metadata": {
        "id": "ol1JCGCXmkqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "auc-YLI0qPmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Export transactions dataframe into CSV\n",
        "\n"
      ],
      "metadata": {
        "id": "Ojxb0cORqS37"
      }
    }
  ]
}