{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWruObKsbZa2ltDRg84j3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "* This project is meant to bring all my personal finance related transactions into one easy to understand view.\n",
        "* Scope / Value descriptoon\n",
        "    * ...\n",
        "* Mechanism\n",
        "    * It reads CSV files from several US and German banks and Credit Card processors and harmonizes them into one dataframe.\n",
        "    * It maps fields such as descriptions into payees\n",
        "        * Lookup mechanism (direct hit and prefix hit) against a payee config JSON file\n",
        "        * Matching against similarity vectors per payee to identify candidates (manual adjustment of payee JSON afterwards)\n",
        "    * It categorizes each transaction or splits it into several categories\n",
        "        * by payee\n",
        "        * by pre-determination of a percentage split (e.g. for Walgreens that should be sufficient, given that I have categorized transactions since 2014)\n",
        "        * by semi-automatic order-item review split (e.g. for Apple or Amazon transactions where these files exist and where a split between utility and subscription or grocery, household products or general shopping is of interest)\n",
        "    * It works with a set of indicator field to mark aspects of interest\n",
        "        * Indicator for transactions in which automatic determinations have been taken place\n",
        "        * Task field to address open tasks\n",
        "        * ..."
      ],
      "metadata": {
        "id": "jp_J4Oz5euR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jDWIT_CWmLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of Libraries\n",
        "* Loading of neccessary libraries such as Pandas etc."
      ],
      "metadata": {
        "id": "YFUcNTnFoxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "754FWtrPPMRN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define global Variables\n",
        "* Create transactions structure that ultimately will hold the transactions dataframes from all bank files\n",
        "* Create overall transactions dataframe"
      ],
      "metadata": {
        "id": "MQ1-J3IcotCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transactions dataframe and load the JSON configuration for the different banks\n",
        "bank_transactions = {}\n",
        "\n",
        "bank_files = {\n",
        "        \"Chase\": \"chase.csv\",\n",
        "        \"Wells Fargo Checking\": \"wellsfargo_checking.csv\",\n",
        "        \"Apple\": \"apple.csv\",\n",
        "        \"Commerzbank\": \"commerzbank.csv\"\n",
        "    }\n",
        "\n",
        "all_transactions = []"
      ],
      "metadata": {
        "id": "8W7j8YnVNfEX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Conversion"
      ],
      "metadata": {
        "id": "DonELYoNmHRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For each bank file:\n",
        "    * Load file into individual df\n",
        "    * Basic quality control on the individual df level\n",
        "    * Transform columns into target columns\n",
        "        * Add Bank ID field as well as numberical ID field\n",
        "    * Add individual df to transactions df\n",
        "\n",
        "* Special transformations for non-US banks:\n",
        "    * Date conversion\n",
        "    * EUR to USD conversion based on an existing file (date and exchange rate or an API call to a free service)"
      ],
      "metadata": {
        "id": "sV_EgVu5pG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_control(df):\n",
        "    missing_values = df.isnull().sum()\n",
        "    column_data_types = df.dtypes\n",
        "\n",
        "    return missing_values, column_data_types"
      ],
      "metadata": {
        "id": "we4_xBVmKZPX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_field_names(df, bank=\"\"):\n",
        "\n",
        "    if 'Category' in df.columns:\n",
        "        df = df.rename(columns={\"Category\" : \"oldCategory\"})\n",
        "\n",
        "    df.insert(4, 'SplitID',\"\")\n",
        "    df.insert(0, 'Date','')\n",
        "    df.insert(1, 'Payee','')\n",
        "    df.insert(2, 'Category Type','')\n",
        "    df.insert(3, 'Category','')\n",
        "    df.insert(4, 'chkPayee','')\n",
        "    df.insert(5, 'chkCategory','')\n",
        "    df.insert(6, 'chkSplit','')\n",
        "    df.insert(7, 'chkEURUSD','')\n",
        "\n",
        "#    if bank == \"Commerzbank\":\n",
        "#        df.insert(\"Amount (USD)\")\n",
        "#        df = df.rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sx688qkjJyLA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wells Fargo"
      ],
      "metadata": {
        "id": "iSkK3j8oYDrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wells Fargo Checking"
      ],
      "metadata": {
        "id": "OKPVENVyYGzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Wells Fargo Checking CSV\n",
        "bank = 'Wells Fargo Checking'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank], header=None, names=[\"Transaction Date\", \"Amount (USD)\", \"Status\", \"Memo\", \"Description\"])\n",
        "\n",
        "# Adjust field names (if any specific adjustments are required)\n",
        "\n",
        "# Convert 'Transaction Date' column to datetime\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "\n",
        "# Check for problematic dates (rows where the date conversion failed)\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "\n",
        "# Perform quality control checks\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "# Final touches for Wells Fargo only\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction Date', 'Status', 'Memo'])  # Assuming 'Status' is not needed, adjust as necessary\n",
        "bank_transactions[bank]['Amount (USD)'] *= -1"
      ],
      "metadata": {
        "id": "3ayfeFcIJeLo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chase"
      ],
      "metadata": {
        "id": "uGfOhoFalRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Chase'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Post Date', 'oldCategory', 'Type', 'Memo', 'Transaction Date'])\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Amount\" : \"Amount (USD)\"})\n"
      ],
      "metadata": {
        "id": "S2SUWsyGYA0d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple"
      ],
      "metadata": {
        "id": "_PiuqQMNlTrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Apple'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank], bank)\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "# Final touches for Apple Card only\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction Date', 'Clearing Date', 'Merchant', 'oldCategory', 'Type', 'Purchased By'])\n",
        "bank_transactions[bank]['Amount (USD)'] *= -1"
      ],
      "metadata": {
        "id": "Fb8FLo8XeVqt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commerzbank"
      ],
      "metadata": {
        "id": "N8bnZFhBlU8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Commerzbank'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction date'], errors='coerce', format='%d.%m.%Y') # For Commerzbank, Day.Month.Year\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank][bank_transactions[bank]['Amount'] != 0]\n",
        "\n",
        "\n",
        "bank_transactions[bank].insert(7, \"Amount (USD)\",\"\")\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "### EUR to USD conversion\n",
        "# https://www.wsj.com/market-data/quotes/fx/EURUSD/historical-prices\n",
        "\n",
        "exchange_rates_data = pd.read_csv('eur_usd_exchange_rates.csv')\n",
        "\n",
        "# Convert the date columns to consistent datetime format\n",
        "exchange_rates_data['Date'] = pd.to_datetime(exchange_rates_data['Date'], format='%m/%d/%Y')\n",
        "\n",
        "# Merge on the date columns to add the exchange rate to bank_transactions[bank]\n",
        "bank_transactions[bank] = bank_transactions[bank].merge(exchange_rates_data[['Date', ' Close']], on='Date', how='left')\n",
        "\n",
        "# Add the chkEURUSD column based on the ' Close' column value\n",
        "bank_transactions[bank]['chkEURUSD'] = np.where(bank_transactions[bank][' Close'].isna(), 'E', 'A')\n",
        "\n",
        "# Convert the Amount from EUR to USD\n",
        "bank_transactions[bank]['Amount (USD)'] = bank_transactions[bank]['Amount'] * bank_transactions[bank][' Close']\n",
        "\n",
        "# Drop the ' Close' column as it's not needed anymore in bank_transactions[bank]\n",
        "bank_transactions[bank].drop(' Close', axis=1, inplace=True)\n",
        "\n",
        "### End of currency conversion\n",
        "\n",
        "#bank_transactions[bank].drop(bank_transactions[bank].columns[[16, 15, 14, 13, 12, 10, 9, 8]], axis=1, inplace=True)\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction date', 'Value date', 'Transaction type', 'Amount', 'Account of initiator', 'Bank code of account of initiator', 'IBAN of account of initiator'])\n"
      ],
      "metadata": {
        "id": "qLtMUpw7e1Wx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_transactions = pd.concat(bank_transactions, keys=bank_transactions.keys())\n",
        "all_transactions['Account-ID'] = all_transactions.index.get_level_values(0) + \"-\" + all_transactions.index.get_level_values(1).astype(str)"
      ],
      "metadata": {
        "id": "W4NvjMwc3FRS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Payees"
      ],
      "metadata": {
        "id": "qvAQCjR02_0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Payee Harmonization"
      ],
      "metadata": {
        "id": "IQ2XK_tVq09c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "class MerchantMatcher:\n",
        "    def __init__(self, data_df):\n",
        "        self.data = data_df\n",
        "        self.vectorizer = self._train_vectorizer()\n",
        "        self.payee_vectors = self._compute_payee_vectors()\n",
        "        self.positive_list_descriptions = self._get_positive_list_descriptions()\n",
        "\n",
        "    def _match_prefix(self, description, merchant_details):\n",
        "        prefix_length = merchant_details.get('Prefix Length', 50)\n",
        "        for known_description in merchant_details['Positive List']:\n",
        "            truncated_payee = known_description.lower()[:prefix_length]\n",
        "            if description.lower().startswith(truncated_payee):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def _train_vectorizer(self):\n",
        "        all_descriptions = [desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions]\n",
        "        return TfidfVectorizer().fit(all_descriptions)\n",
        "\n",
        "    def _compute_payee_vectors(self):\n",
        "        payee_vectors = {}\n",
        "        for merchant, details in self.data.iterrows():\n",
        "            tfidf_matrix = self.vectorizer.transform([desc.lower() for desc in details['Positive List']])\n",
        "            avg_vector = np.asarray(tfidf_matrix.mean(axis=0))\n",
        "            payee_vectors[merchant] = avg_vector\n",
        "        return payee_vectors\n",
        "\n",
        "    def _get_positive_list_descriptions(self):\n",
        "        return set(desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions)\n",
        "\n",
        "    def predict_payees(self, transaction_df):\n",
        "        mg_values = []\n",
        "        chkpayee_values = []\n",
        "        candidates = []\n",
        "\n",
        "        for _, row in transaction_df.iterrows():\n",
        "            description_lower = row['Description'].lower() if row['Description'] else None\n",
        "            current_merchant = None\n",
        "            current_chkpayee = None\n",
        "\n",
        "            if pd.isna(description_lower) or not description_lower.strip():\n",
        "                mg_values.append(None)\n",
        "                chkpayee_values.append(None)\n",
        "                continue\n",
        "\n",
        "            for merchant, details in self.data.iterrows():\n",
        "                if description_lower in [desc.lower() for desc in details['Positive List']]:\n",
        "                    current_merchant = merchant\n",
        "                    current_chkpayee = 'A'\n",
        "                    break\n",
        "\n",
        "                # Check for prefix matching\n",
        "                if self._match_prefix(description_lower, details):\n",
        "                    current_merchant = merchant\n",
        "                    current_chkpayee = 'P'\n",
        "                    break\n",
        "\n",
        "            if not current_merchant:\n",
        "                description_vector = self.vectorizer.transform([description_lower])\n",
        "                similarities = {merchant: linear_kernel(description_vector, np.asarray(vector))[0][0] for merchant, vector in self.payee_vectors.items()}\n",
        "                predicted_merchant = max(similarities, key=similarities.get)\n",
        "                max_similarity = similarities[predicted_merchant]\n",
        "\n",
        "                if max_similarity > self.data.loc[predicted_merchant, 'Threshold']:\n",
        "                    candidates.append({'Payee': predicted_merchant, 'Description': row['Description'], 'Probability': max_similarity})\n",
        "\n",
        "            mg_values.append(current_merchant)\n",
        "            chkpayee_values.append(current_chkpayee or 'C')\n",
        "\n",
        "        transaction_df['Payee'] = mg_values\n",
        "        transaction_df['chkPayee'] = chkpayee_values\n",
        "        candidates_df = pd.DataFrame(candidates)\n",
        "        return transaction_df, candidates_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sample Usage\n",
        "data_df = pd.read_json(\"payee_matching.json\", orient=\"index\")  # Replace with your DataFrame loading mechanism\n",
        "\n",
        "matcher = MerchantMatcher(data_df)\n",
        "payees_identified_df, payees_candidates_df = matcher.predict_payees(all_transactions)\n",
        "payees_identified_df = payees_identified_df[payees_identified_df['chkPayee'] != 'C']\n",
        "\n",
        "file_payees_identified = \"z_payees_identified.csv\"\n",
        "file_payees_candidates = \"z_payees_candidates.csv\"\n",
        "\n",
        "if os.path.exists(file_payees_identified): os.remove(file_payees_identified)\n",
        "if os.path.exists(file_payees_candidates): os.remove(file_payees_candidates)\n",
        "payees_identified_df.to_csv(file_payees_identified, index=False)\n",
        "payees_candidates_df.to_csv(file_payees_candidates, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_Ltgm3iSlk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categories"
      ],
      "metadata": {
        "id": "DF0bxKmWmsz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transactions <--> Payee mapping (1:1)\n",
        "* Transactions <--> Amazon Orders mapping and splitting\n",
        "* Transactions <--> Apple Orders mapping and splitting\n",
        "* Transactions <--> Walgreens splitting"
      ],
      "metadata": {
        "id": "ol1JCGCXmkqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct assignment"
      ],
      "metadata": {
        "id": "u4CttrmftPPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transactions <--> Payee mapping (1:1)\n",
        "\n",
        "with open('payee_matching.json', 'r') as file:\n",
        "    payee_data = json.load(file)\n",
        "\n",
        "# List to hold split transactions\n",
        "split_transactions = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for idx, row in all_transactions.iterrows():\n",
        "    payee = row['Payee']\n",
        "\n",
        "    # Check if payee exists in the JSON data\n",
        "    if payee in payee_data:\n",
        "        categories = payee_data[payee]['Categories']\n",
        "\n",
        "        # If no category exists, update the row's category columns\n",
        "        if len(categories) == 0:\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'E'\n",
        "\n",
        "        # If only one category exists, update the row's category columns\n",
        "        if len(categories) == 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = categories[0]['Category Type']\n",
        "            all_transactions.at[idx, 'Category'] = categories[0]['Category']\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "        # If multiple categories exist, create split transactions\n",
        "        elif len(categories) > 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = ''  # Empty the master row's category columns\n",
        "            all_transactions.at[idx, 'Category'] = ''\n",
        "            all_transactions.at[idx, 'SplitID'] = str(row['Account-ID']) + '-' + 'M'\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "            for idx_split, category in enumerate(categories, start=1):\n",
        "                new_row = row.copy()\n",
        "                new_row['Category Type'] = category['Category Type']\n",
        "                new_row['Category'] = category['Category']\n",
        "                new_row['SplitID'] = str(row['Account-ID']) + '-' + 'S' + str(idx_split-1)\n",
        "                new_row['chkCategory'] = 'A'\n",
        "\n",
        "                # Update the 'Amount (USD)' based on the percentage split from the JSON\n",
        "                new_row['Amount (USD)'] = row['Amount (USD)'] * category.get('Percentage', 1)\n",
        "\n",
        "                split_transactions.append(new_row)\n",
        "\n",
        "# Append the split transactions to the main dataframe\n",
        "all_transactions = pd.concat([all_transactions, pd.DataFrame(split_transactions)], ignore_index=False)"
      ],
      "metadata": {
        "id": "s4LpNZWV_dgB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amazon categorization\n",
        "\n",
        "1. **Identification of Amazon Transactions**:\n",
        "* Filter transactions with the Payee set to \"Amazon\" or \"Amazon Grocery\".\n",
        "* From this subset, take those transactions that don't already have a chkCategory flag.\n",
        "   \n",
        "2. **Match the Transactions to Orders**:\n",
        "   - For each identified Amazon transaction, we need to match it with an order from the Amazon order file. This matching will be based on the transaction date (with a tolerance of a few days) and the payment amount.\n",
        "   \n",
        "3. **Extract Items for the Matched Orders**:\n",
        "   - Once we have identified the matching order, we will then look up the items related to that order from the Amazon order items file.\n",
        "   \n",
        "4. **Categorize the Items**:\n",
        "   - We will categorize the items into two groups:\n",
        "     - Groceries (Split 1)\n",
        "     - All other line items (Split 2...n)\n",
        "   \n",
        "5. **Modify the Transactions**:\n",
        "   - We will then modify the transactions to reflect these splits, updating the description for each split with the appropriate line item description."
      ],
      "metadata": {
        "id": "tiI3oyFtzzf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Split and Identify Payments\n",
        "1.1. Split the payments up from the Amazon header data.\n",
        "1.2. Identify and match the transactions associated with these individual payments.\n",
        "1.3. Retire the identified transactions and replace them with a new, merged transaction that sums up these payments. This merged transaction will have the chkmerged marker and the associated order ID.\n",
        "\n",
        "Step 2: Categorize Items\n",
        "2.1. For each item in an order, identify its category as we've done before.\n",
        "\n",
        "Step 3: Compute Total for Each Order\n",
        "3.1. For each item, calculate the sum of the product of quantity and price. If quantity is NaN, assume it to be 1.\n",
        "3.2. Add taxes and shipping, and subtract the gift card amount. The result is the computed total for the order.\n",
        "3.3. Compare this computed total with the calculated overall payment for the order from the merged transaction. These two values should match.\n",
        "\n",
        "Step 4: Create Splits for Each Item\n",
        "4.1. Make the merged transaction the master transaction.\n",
        "4.2. Associate each item as a split transaction, where the split amount is the item price plus a portion of taxes, shipping, etc.\n",
        "4.3. The splits should add up to match the master transaction amount.\n",
        "\n",
        "Step 5: Roll-Up Splits by Category\n",
        "5.1. Group the splits by their category for each order.\n",
        "5.2. Sum up the amounts within these groups.\n",
        "5.3. Retain only one split per category, with the summed amount.\n",
        "5.4. These rolled-up splits should still add up to match the master transaction amount."
      ],
      "metadata": {
        "id": "UXr0bRUiJKIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going forward plan\n",
        "\n",
        "Thank you for the comprehensive code. Let's go through it step by step and re-engineer based on our new plan:\n",
        "\n",
        "1. AmazonProcessor (Cell 1)\n",
        "This class is responsible for processing the Amazon order headers, particularly to split multiple payments.\n",
        "\n",
        "**Usefulness**: This class is essential. We need to split multiple payments to associate them with individual transactions.\n",
        "\n",
        "2. AmazonTransactionMatcher (Cell 2)\n",
        "This class matches the processed Amazon payments (from the AmazonProcessor) with the actual bank transactions.\n",
        "\n",
        "**Usefulness**: This class is crucial. After processing the Amazon headers, we need to match them to actual transactions.\n",
        "\n",
        "3. CategoryIdentification (Cell 3)\n",
        "This class categorizes each item based on a keyword list.\n",
        "\n",
        "**Usefulness**: Still essential. We want to identify each item's category.\n",
        "\n",
        "4. AmazonProcessor (Cell 4)\n",
        "This class processes the merged DataFrame (`final_df`) to handle grocery and non-grocery splits and integrate them into the `all_transactions`.\n",
        "\n",
        "**Usefulness**: This needs restructuring. The logic here is to be modified as per our new approach.\n",
        "\n",
        "---\n",
        "\n",
        "Proposed Structure:\n",
        "1. **AmazonPaymentProcessor**: Refactor `AmazonProcessor` from Cell 1. This will process the Amazon headers and split multiple payments.\n",
        "   \n",
        "2. **TransactionMatcher**: Refactor `AmazonTransactionMatcher` from Cell 2. This will match Amazon payments to actual transactions. After matching, create a single merged transaction for each order, marking it with `chkmerged`.\n",
        "\n",
        "3. **ItemCategorizer**: Refactor `CategoryIdentification` from Cell 3. This class will assign categories to items.\n",
        "\n",
        "4. **TransactionUpdater**: Create a new class (replacing `AmazonProcessor` from Cell 4). This class will:\n",
        "    - Calculate the total for each order.\n",
        "    - Check the totals against the merged transaction values.\n",
        "    - Create splits for each item.\n",
        "    - Roll up splits by category.\n",
        "\n",
        "---\n",
        "\n",
        "Steps for Implementation:\n",
        "\n",
        "1. Rename and restructure `AmazonProcessor` from Cell 1 to `AmazonPaymentProcessor`.\n",
        "   \n",
        "2. Rename and restructure `AmazonTransactionMatcher` from Cell 2 to `TransactionMatcher`. Modify it to create the merged transactions.\n",
        "\n",
        "3. Rename and slightly modify `CategoryIdentification` from Cell 3 to `ItemCategorizer`.\n",
        "\n",
        "4. Replace `AmazonProcessor` from Cell 4 with a new class named `TransactionUpdater`. Modify the logic as per our new approach.\n",
        "\n",
        "Would you like to proceed with this structure?"
      ],
      "metadata": {
        "id": "8utsf0lWKBPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon Payment Processor"
      ],
      "metadata": {
        "id": "VO10hFL8P3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "class AmazonPaymentProcessor:\n",
        "    def __init__(self, orders_headers, order_items=None, grocery_keywords=None):\n",
        "        self.orders_headers = orders_headers\n",
        "        self.order_items = order_items\n",
        "        self.grocery_keywords = grocery_keywords\n",
        "        self.processed_orders = self.split_multiple_payments(orders_headers)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_date_format(date_str):\n",
        "        pattern = r'(\\d{1,2})\\s*(\\d{1,2})\\s*(\\d{4})'\n",
        "        match = re.match(pattern, date_str)\n",
        "        if match:\n",
        "            month, day, year = match.groups()\n",
        "            return f\"{year}-{int(month):02d}-{int(day):02d}\"\n",
        "        else:\n",
        "            return date_str\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_multiple_payments(payment_string):\n",
        "        pattern = r\"(?:MasterCard|Visa)?(?:\\s*ending\\s*in\\s*\\d{4})?:?\\s*([A-Za-z]*\\s*\\d{1,2},?\\s*\\d{4}|\\d{4}-\\d{1,2}-\\d{1,2})[^$]*\\$\\s*([\\d,]+\\.\\d{2})\"\n",
        "        matches = re.findall(pattern, payment_string)\n",
        "\n",
        "        month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "        month_map = {month: str(index + 1) for index, month in enumerate(month_names)}\n",
        "\n",
        "        processed_matches = []\n",
        "        for date, amount in matches:\n",
        "            for month, month_num in month_map.items():\n",
        "                date = date.replace(month, month_num)\n",
        "            date = date.replace(\",\", \"\").replace(\" \", \"-\")\n",
        "            date = AmazonProcessor.normalize_date_format(date)  # normalize the date format here\n",
        "            processed_matches.append((date, float(amount.replace(',', ''))))\n",
        "\n",
        "        return processed_matches\n",
        "\n",
        "    def split_multiple_payments(self, df):\n",
        "        new_rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            payments = self.extract_multiple_payments(row['payments'])\n",
        "            for date, amount in payments:\n",
        "                new_row = row.copy()\n",
        "                new_row['payment_date'] = date\n",
        "                new_row['payment_amount'] = amount\n",
        "                new_rows.append(new_row)\n",
        "        return pd.DataFrame(new_rows)\n",
        "\n",
        "    def get_processed_orders(self):\n",
        "        return self.processed_orders.copy()\n",
        "\n",
        "# Sample Usage in Google Colab:\n",
        "# Assuming you've uploaded 'amazon_axel_orders_headers_jul_aug.csv'\n",
        "orders_headers_df = pd.read_csv(\"/content/amazon_axel_orders_headers_jul_aug.csv\")\n",
        "amazon_payment_processor = AmazonPaymentProcessor(orders_headers_df)\n",
        "order_data_processed = amazon_payment_processor.get_processed_orders()\n",
        "\n",
        "order_data_processed = order_data_processed.drop(columns=['items', 'to'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "G05g5zt07HT0",
        "outputId": "d5a90e4e-c2a8-4408-dd8f-4ff47a089a59"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-095670322fd5>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Assuming you've uploaded 'amazon_axel_orders_headers_jul_aug.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0morders_headers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/amazon_axel_orders_headers_jul_aug.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mamazon_payment_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazonPaymentProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_headers_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0morder_data_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamazon_payment_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processed_orders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-095670322fd5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, orders_headers, order_items, grocery_keywords)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrocery_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrocery_keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_orders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_multiple_payments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-095670322fd5>\u001b[0m in \u001b[0;36msplit_multiple_payments\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mpayments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_multiple_payments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-095670322fd5>\u001b[0m in \u001b[0;36mextract_multiple_payments\u001b[0;34m(payment_string)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazonProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_date_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize the date format here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprocessed_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'AmazonProcessor' has no attribute 'normalize_date_format'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transaction Matcher"
      ],
      "metadata": {
        "id": "pxxoICcdP8n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonTransactionMatcher:\n",
        "    def __init__(self, order_data, transaction_data):\n",
        "        self.order_data = order_data\n",
        "        self.transaction_data = transaction_data\n",
        "        self.matches = pd.DataFrame()\n",
        "\n",
        "    def match_transactions(self):\n",
        "        # Convert 'payment_date' in order_data to datetime format\n",
        "        self.order_data['payment_date'] = pd.to_datetime(self.order_data['payment_date'])\n",
        "\n",
        "        # Convert 'payment_amount' in order_data to float\n",
        "        self.order_data['payment_amount'] = self.order_data['payment_amount'].astype(float)\n",
        "\n",
        "        # Merge based on payment_date and absolute value of payment_amount\n",
        "        self.matches = pd.merge(\n",
        "            left=self.transaction_data,\n",
        "            right=self.order_data,\n",
        "            left_on=['Date'],\n",
        "            right_on=['payment_date'],\n",
        "            how='inner'\n",
        "        ).assign(matched=lambda x: abs(x['Amount (USD)']) == x['payment_amount'])\n",
        "\n",
        "        # Filter out rows where the absolute value of Amount (USD) isn't equal to payment_amount\n",
        "        self.matches = self.matches[self.matches['matched']]\n",
        "        self.matches.drop(columns=['matched'], inplace=True)\n",
        "\n",
        "    def get_matched_transactions(self):\n",
        "        return self.matches.copy()\n",
        "\n",
        "\n",
        "# Assuming 'all_transactions' is the DataFrame containing your transactions\n",
        "matcher = AmazonTransactionMatcher(order_data_processed, all_transactions)\n",
        "matcher.match_transactions()\n",
        "matched_transactions = matcher.get_matched_transactions()\n",
        "#print(matched_transactions.head())\n",
        "\n",
        "matched_transactions"
      ],
      "metadata": {
        "id": "mpGyBmcHC38K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "class CategoryIdentification:\n",
        "    def __init__(self, order_items_df, keyword_df):\n",
        "        self.order_items = order_items_df\n",
        "        self.keywords = keyword_df\n",
        "        self.candidates = []\n",
        "\n",
        "    def identify_category(self, item_description):\n",
        "        # Direct keyword match\n",
        "        matched_category = self.keywords[self.keywords['keyword'].str.lower() == item_description.lower()]['category'].values\n",
        "\n",
        "        # If a match is found, return the category\n",
        "        if matched_category.size > 0:\n",
        "            return matched_category[0]\n",
        "\n",
        "        # If no match is found, add to candidates and return None\n",
        "        self.candidates.append(item_description)\n",
        "        return None\n",
        "\n",
        "    def categorize_order_items(self):\n",
        "        self.order_items['category'] = self.order_items['description'].apply(self.identify_category)\n",
        "        return self.order_items\n",
        "\n",
        "    def get_uncategorized_items(self):\n",
        "        return self.candidates\n",
        "\n",
        "\n",
        "order_items_df = pd.read_csv(\"amazon_axel_order_items_jul_aug.csv\")  # Read your Amazon order items CSV\n",
        "keyword_df = pd.read_csv(\"amazon_product_keywords.csv\")  # Read your keyword CSV\n",
        "category_identifier = CategoryIdentification(order_items_df, keyword_df)\n",
        "\n",
        "categorized_order_items = category_identifier.categorize_order_items()\n",
        "\n",
        "# Rename the description column in matched_transactions to avoid conflict\n",
        "matched_transactions = matched_transactions.rename(columns={'description': 'transaction_description'})\n",
        "\n",
        "# Merge the dataframes\n",
        "final_df = pd.merge(matched_transactions, categorized_order_items[['order id', 'description', 'category', 'price', 'quantity']], on=\"order id\", how=\"left\")\n",
        "\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items, on=\"order id\", how=\"left\")\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items[['order id', 'description', 'category']], on=\"order id\", how=\"left\")\n",
        "\n",
        "\n",
        "uncategorized_items = category_identifier.get_uncategorized_items()\n",
        "\n",
        "final_df = final_df[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'payment_date',\n",
        "    'payment_amount',\n",
        "    'category',\n",
        "    'Description',\n",
        "    'description',\n",
        "    'price',\n",
        "    'quantity',\n",
        "    'total',\n",
        "    'shipping',\n",
        "    'shipping_refund',\n",
        "    'gift',\n",
        "    'tax',\n",
        "    'refund',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "\n",
        "if os.path.exists(\"final_df.csv\"): os.remove(\"final_df.csv\")\n",
        "final_df.to_csv(\"final_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "upsTJv6_QCeA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonProcessor:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df.sort_values(by='order id')\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def modify_all_transactions(self):\n",
        "        # Step 1: Remove Amazon-related transactions from the all_transactions DataFrame\n",
        "        self.all_transactions = self.all_transactions[~self.all_transactions['Payee'].str.contains('Amazon', na=False)]\n",
        "\n",
        "        modified_amazon_transactions = []\n",
        "\n",
        "        # Step 2: Process Amazon Transactions\n",
        "        for order_id, group in self.final_df.groupby('order id'):\n",
        "            main_transaction = group.iloc[0]  # Use the first row for the main transaction details\n",
        "\n",
        "            # Main transaction row: ensure it has all columns from all_transactions\n",
        "            main_row = main_transaction.to_dict()\n",
        "            modified_amazon_transactions.append(main_row)\n",
        "\n",
        "            # Splits for grocery items\n",
        "            grocery_rows = group[group['category'] == 'grocery']\n",
        "            if not grocery_rows.empty:\n",
        "                total_grocery_amount = grocery_rows['price'].sum()\n",
        "                grocery_row = main_transaction.copy()\n",
        "                grocery_row['Payee'] = 'Split Transaction'\n",
        "                grocery_row['Description'] = 'Amazon Groceries'\n",
        "                grocery_row['Amount (USD)'] = total_grocery_amount\n",
        "                modified_amazon_transactions.append(grocery_row)\n",
        "\n",
        "            # Splits for non-grocery items\n",
        "            non_grocery_rows = group[group['category'] != 'grocery']\n",
        "            for _, row in non_grocery_rows.iterrows():\n",
        "                non_grocery_row = row.copy()\n",
        "                non_grocery_row['Payee'] = 'Split Transaction'\n",
        "                non_grocery_row['Description'] = row['description']\n",
        "                non_grocery_row['Amount'] = row['price']\n",
        "                modified_amazon_transactions.append(non_grocery_row)\n",
        "\n",
        "        # Step 3: Append the modified Amazon transactions back to all_transactions\n",
        "        amazon_df = pd.DataFrame(modified_amazon_transactions)\n",
        "        self.all_transactions = pd.concat([self.all_transactions, amazon_df], ignore_index=True)\n",
        "\n",
        "        return self.all_transactions\n",
        "\n",
        "processor = AmazonProcessor(final_df, all_transactions)\n",
        "updated_all_transactions = processor.modify_all_transactions()\n",
        "\n",
        "processor = AmazonProcessor(final_df, all_transactions)\n",
        "updated_all_transactions = processor.modify_all_transactions()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(\"final_df.csv\"): os.remove(\"final_df.csv\")\n",
        "updated_all_transactions.to_csv(\"final_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "YfCn8JbK2iU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "f662de9b-1262-419c-a07d-6481fd8215e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c88e782e6b47>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazonProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_transactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mupdated_all_transactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodify_all_transactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazonProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_transactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-c88e782e6b47>\u001b[0m in \u001b[0;36mmodify_all_transactions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mnon_grocery_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Payee'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Split Transaction'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mnon_grocery_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mnon_grocery_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mmodified_amazon_transactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_grocery_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "auc-YLI0qPmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe preparation"
      ],
      "metadata": {
        "id": "Ojxb0cORqS37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder Columns\n",
        "\n",
        "all_transactions = all_transactions[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Description',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "# Sort rows\n",
        "all_transactions = all_transactions.sort_values(by=['Date', 'Account-ID', 'SplitID'], ascending=[False, True, True])\n",
        "\n",
        "# Formating\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].round(2)\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].apply(lambda x: \"${:,.2f}\".format(x))\n"
      ],
      "metadata": {
        "id": "EJTopk0eo8Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output file generation"
      ],
      "metadata": {
        "id": "tbGNVP1OrelH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"z_output.csv\"): os.remove(\"z_output.csv\")\n",
        "all_transactions.to_csv(\"z_output.csv\", index=False)"
      ],
      "metadata": {
        "id": "glF2a7uTBl9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}