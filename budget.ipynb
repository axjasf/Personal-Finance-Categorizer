{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxEGtePfLJejEew0gRO8w6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "* This project is meant to bring all my personal finance related transactions into one easy to understand view.\n",
        "* Scope / Value descriptoon\n",
        "    * ...\n",
        "* Mechanism\n",
        "    * It reads CSV files from several US and German banks and Credit Card processors and harmonizes them into one dataframe.\n",
        "    * It maps fields such as descriptions into payees\n",
        "        * Lookup mechanism (positive and negative lists) against a payee config JSON file\n",
        "        * Fuzzy matching against pre-determined patterns\n",
        "    * It categorizes each transaction or splits it into several categories\n",
        "        * by payee\n",
        "        * by pre-determination of a percentage split (e.g. for Walgreens that should be sufficient, given that I have categorized transactions since 2014)\n",
        "        * by semi-automatic order-item review split (e.g. for Apple or Amazon transactions where these files exist and where a split between utility and subscription or grocery, household products or general shopping is of interest)\n",
        "    * It works with a set of indicator field to mark aspects of interest\n",
        "        * Indicator for transactions in which automatic determinations have been taken place\n",
        "        * Task field to address open tasks\n",
        "        * ..."
      ],
      "metadata": {
        "id": "jp_J4Oz5euR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jDWIT_CWmLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Libraries\n",
        "\n",
        "*   Neccessary libraries that might not be available right away in CoLab need to be installed here.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EvOqV2xgovT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of Libraries\n",
        "* Loading of neccessary libraries such as Pandas etc."
      ],
      "metadata": {
        "id": "YFUcNTnFoxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "754FWtrPPMRN"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define global Variables\n",
        "* Create transactions structure that ultimately will hold the transactions dataframes from all bank files\n",
        "* Create overall transactions dataframe"
      ],
      "metadata": {
        "id": "MQ1-J3IcotCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transactions dataframe and load the JSON configuration for the different banks\n",
        "bank_transactions = {}\n",
        "\n",
        "bank_files = {\n",
        "        \"Chase\": \"chase.csv\",\n",
        "        \"Wells Fargo Checking\": \"wells_fargo_checking.csv\",\n",
        "        \"Apple\": \"apple.csv\",\n",
        "        \"Commerzbank\": \"commerzbank.csv\"\n",
        "    }\n",
        "\n",
        "all_transactions = []"
      ],
      "metadata": {
        "id": "8W7j8YnVNfEX"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Conversion"
      ],
      "metadata": {
        "id": "DonELYoNmHRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For each bank file:\n",
        "    * Load file into individual df\n",
        "    * Basic quality control on the individual df level\n",
        "    * Transform columns into target columns\n",
        "        * Add Bank ID field as well as numberical ID field\n",
        "    * Add individual df to transactions df\n",
        "\n",
        "* Special transformations for non-US banks:\n",
        "    * Date conversion\n",
        "    * EUR to USD conversion based on an existing file (date and exchange rate or an API call to a free service)"
      ],
      "metadata": {
        "id": "sV_EgVu5pG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_control(df):\n",
        "    missing_values = df.isnull().sum()\n",
        "    column_data_types = df.dtypes\n",
        "\n",
        "    return missing_values, column_data_types"
      ],
      "metadata": {
        "id": "we4_xBVmKZPX"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_field_names(df, bank=\"\"):\n",
        "\n",
        "    if 'Category' in df.columns:\n",
        "        df = df.rename(columns={\"Category\" : \"oldCategory\"})\n",
        "\n",
        "    df.insert(4, 'SplitID',\"\")\n",
        "    df.insert(0, 'Date','')\n",
        "    df.insert(1, 'Payee','')\n",
        "    df.insert(2, 'Category Type','')\n",
        "    df.insert(3, 'Category','')\n",
        "    df.insert(4, 'chkPayee','')\n",
        "    df.insert(5, 'chkCategory','')\n",
        "    df.insert(6, 'chkSplit','')\n",
        "    df.insert(7, 'chkEURUSD','')\n",
        "\n",
        "#    if bank == \"Commerzbank\":\n",
        "#        df.insert(\"Amount (USD)\")\n",
        "#        df = df.rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sx688qkjJyLA"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chase"
      ],
      "metadata": {
        "id": "uGfOhoFalRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Chase'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Post Date', 'oldCategory', 'Type', 'Memo', 'Transaction Date'])\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Amount\" : \"Amount (USD)\"})\n"
      ],
      "metadata": {
        "id": "3ayfeFcIJeLo"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple"
      ],
      "metadata": {
        "id": "_PiuqQMNlTrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Apple'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank], bank)\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction Date', 'Clearing Date', 'Merchant', 'oldCategory', 'Type', 'Purchased By'])"
      ],
      "metadata": {
        "id": "Fb8FLo8XeVqt"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commerzbank"
      ],
      "metadata": {
        "id": "N8bnZFhBlU8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Commerzbank'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction date'], errors='coerce', format='%d.%m.%Y') # For Commerzbank, Day.Month.Year\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank][bank_transactions[bank]['Amount'] != 0]\n",
        "\n",
        "\n",
        "bank_transactions[bank].insert(7, \"Amount (USD)\",\"\")\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "### https://www.wsj.com/market-data/quotes/fx/EURUSD/historical-prices\n",
        "\n",
        "exchange_rates_data = pd.read_csv('eur_usd_exchange_rates.csv')\n",
        "\n",
        "# Convert the date columns to consistent datetime format\n",
        "exchange_rates_data['Date'] = pd.to_datetime(exchange_rates_data['Date'], format='%m/%d/%Y')\n",
        "\n",
        "# Merge on the date columns to add the exchange rate to bank_transactions[bank]\n",
        "bank_transactions[bank] = bank_transactions[bank].merge(exchange_rates_data[['Date', ' Close']], on='Date', how='left')\n",
        "\n",
        "# Convert the Amount from EUR to USD\n",
        "bank_transactions[bank]['Amount (USD)'] = bank_transactions[bank]['Amount'] * bank_transactions[bank][' Close']\n",
        "\n",
        "# Drop the ' Close' column as it's not needed anymore in bank_transactions[bank]\n",
        "bank_transactions[bank].drop(' Close', axis=1, inplace=True)\n",
        "\n",
        "#bank_transactions[bank].drop(bank_transactions[bank].columns[[16, 15, 14, 13, 12, 10, 9, 8]], axis=1, inplace=True)\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction date', 'Value date', 'Transaction type', 'Amount', 'Account of initiator', 'Bank code of account of initiator', 'IBAN of account of initiator'])\n"
      ],
      "metadata": {
        "id": "qLtMUpw7e1Wx"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_transactions = pd.concat(bank_transactions, keys=bank_transactions.keys())\n",
        "all_transactions['Account-ID'] = all_transactions.index.get_level_values(0) + \"-\" + all_transactions.index.get_level_values(1).astype(str)"
      ],
      "metadata": {
        "id": "W4NvjMwc3FRS"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Payees"
      ],
      "metadata": {
        "id": "qvAQCjR02_0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Payee Harmonization"
      ],
      "metadata": {
        "id": "IQ2XK_tVq09c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* New Payee identification\n",
        "    * Match payee against in a config file for payees\n",
        "* Payee transformation"
      ],
      "metadata": {
        "id": "h3dHnqhpq5ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "class MerchantMatcher:\n",
        "    def __init__(self, data_df):\n",
        "        self.data = data_df\n",
        "        self.vectorizer = self._train_vectorizer()\n",
        "        self.payee_vectors = self._compute_payee_vectors()\n",
        "        self.positive_list_descriptions = self._get_positive_list_descriptions()\n",
        "\n",
        "    def _match_prefix(self, description, merchant_details):\n",
        "        prefix_length = merchant_details.get('Prefix Length', 50)\n",
        "        short_description = description[:prefix_length].lower()\n",
        "        positive_list = [desc[:prefix_length].lower() for desc in merchant_details['Positive List']]\n",
        "\n",
        "        for payee_description in positive_list:\n",
        "            if short_description.startswith(payee_description):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _train_vectorizer(self):\n",
        "        all_descriptions = [desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions]\n",
        "        return TfidfVectorizer().fit(all_descriptions)\n",
        "\n",
        "    def _compute_payee_vectors(self):\n",
        "        payee_vectors = {}\n",
        "        for merchant, details in self.data.iterrows():\n",
        "            tfidf_matrix = self.vectorizer.transform([desc.lower() for desc in details['Positive List']])\n",
        "            avg_vector = np.asarray(tfidf_matrix.mean(axis=0))\n",
        "            payee_vectors[merchant] = avg_vector\n",
        "        return payee_vectors\n",
        "\n",
        "    def _get_positive_list_descriptions(self):\n",
        "        return set(desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions)\n",
        "\n",
        "    def predict_payees(self, transaction_df):\n",
        "        mg_values = []\n",
        "        chkpayee_values = []\n",
        "        candidates = []\n",
        "\n",
        "        for _, row in transaction_df.iterrows():\n",
        "            description_lower = row['Description'].lower() if row['Description'] else None\n",
        "            current_merchant = None\n",
        "            current_chkpayee = None\n",
        "\n",
        "            if pd.isna(description_lower) or not description_lower.strip():\n",
        "                mg_values.append(None)\n",
        "                chkpayee_values.append(None)\n",
        "                continue\n",
        "\n",
        "            for merchant, details in self.data.iterrows():\n",
        "                if description_lower in [desc.lower() for desc in details['Positive List']]:\n",
        "\n",
        "                    current_merchant = merchant\n",
        "                    current_chkpayee = 'A'\n",
        "                    break\n",
        "\n",
        "                # Check for prefix matching\n",
        "                for payee_description in details['Positive List']:\n",
        "                    if len(payee_description) > details.get('Prefix Length', 50) and \\\n",
        "                        description_lower.startswith(payee_description[:details.get('Prefix Length', 50)].lower()):\n",
        "                        current_merchant = merchant\n",
        "                        current_chkpayee = 'P'\n",
        "                        break\n",
        "\n",
        "                if current_merchant:\n",
        "                    break\n",
        "\n",
        "            if not current_merchant:\n",
        "                description_vector = self.vectorizer.transform([description_lower])\n",
        "                similarities = {merchant: linear_kernel(description_vector, np.asarray(vector))[0][0] for merchant, vector in self.payee_vectors.items()}\n",
        "                predicted_merchant = max(similarities, key=similarities.get)\n",
        "                max_similarity = similarities[predicted_merchant]\n",
        "\n",
        "                if max_similarity > self.data.loc[predicted_merchant, 'Threshold']:\n",
        "                    candidates.append({'Payee': predicted_merchant, 'Description': row['Description'], 'Probability': max_similarity})\n",
        "\n",
        "\n",
        "            mg_values.append(current_merchant)\n",
        "            chkpayee_values.append(current_chkpayee or 'C')\n",
        "\n",
        "        transaction_df['Payee'] = mg_values\n",
        "        transaction_df['chkPayee'] = chkpayee_values\n",
        "        candidates_df = pd.DataFrame(candidates)\n",
        "        return transaction_df, candidates_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sample Usage\n",
        "data_df = pd.read_json(\"payee_matching.json\", orient=\"index\")  # Replace with your DataFrame loading mechanism\n",
        "\n",
        "matcher = MerchantMatcher(data_df)\n",
        "predicted_df, candidates_df = matcher.predict_payees(all_transactions)\n",
        "\n",
        "if os.path.exists(\"merchant_guess.csv\"): os.remove(\"merchant_guess.csv\")\n",
        "if os.path.exists(\"candidates.csv\"): os.remove(\"candidates.csv\")\n",
        "predicted_df.to_csv(\"merchant_guess.csv\", index=False)\n",
        "candidates_df.to_csv(\"candidates.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_Ltgm3iSlk"
      },
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categories"
      ],
      "metadata": {
        "id": "DF0bxKmWmsz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transactions <--> Payee mapping (1:1)\n",
        "* Transactions <--> Amazon Orders mapping and splitting\n",
        "* Transactions <--> Apple Orders mapping and splitting\n",
        "* Transactions <--> Walgreens splitting"
      ],
      "metadata": {
        "id": "ol1JCGCXmkqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct assignment"
      ],
      "metadata": {
        "id": "u4CttrmftPPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transactions <--> Payee mapping (1:1)\n",
        "\n",
        "with open('payee_matching.json', 'r') as file:\n",
        "    payee_data = json.load(file)\n",
        "\n",
        "# List to hold split transactions\n",
        "split_transactions = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for idx, row in all_transactions.iterrows():\n",
        "    payee = row['Payee']\n",
        "\n",
        "    # Check if payee exists in the JSON data\n",
        "    if payee in payee_data:\n",
        "        categories = payee_data[payee]['Categories']\n",
        "\n",
        "        # If no category exists, update the row's category columns\n",
        "        if len(categories) == 0:\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'E'\n",
        "\n",
        "        # If only one category exists, update the row's category columns\n",
        "        if len(categories) == 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = categories[0]['Category Type']\n",
        "            all_transactions.at[idx, 'Category'] = categories[0]['Category']\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "        # If multiple categories exist, create split transactions\n",
        "        elif len(categories) > 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = ''  # Empty the master row's category columns\n",
        "            all_transactions.at[idx, 'Category'] = ''\n",
        "            all_transactions.at[idx, 'SplitID'] = str(row['Account-ID']) + '-' + 'M'\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "            for idx_split, category in enumerate(categories, start=1):\n",
        "                new_row = row.copy()\n",
        "                new_row['Category Type'] = category['Category Type']\n",
        "                new_row['Category'] = category['Category']\n",
        "                new_row['SplitID'] = str(row['Account-ID']) + '-' + 'S' + str(idx_split-1)\n",
        "                new_row['chkCategory'] = 'A'\n",
        "\n",
        "                # Update the 'Amount (USD)' based on the percentage split from the JSON\n",
        "                new_row['Amount (USD)'] = row['Amount (USD)'] * category.get('Percentage', 1)\n",
        "\n",
        "                split_transactions.append(new_row)\n",
        "\n",
        "# Append the split transactions to the main dataframe\n",
        "all_transactions = pd.concat([all_transactions, pd.DataFrame(split_transactions)], ignore_index=False)"
      ],
      "metadata": {
        "id": "s4LpNZWV_dgB"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_transactions"
      ],
      "metadata": {
        "id": "Ka1UjGrdwQqH"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "auc-YLI0qPmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe preparation"
      ],
      "metadata": {
        "id": "Ojxb0cORqS37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder Columns\n",
        "\n",
        "all_transactions = all_transactions[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Description',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "# Sort rows\n",
        "all_transactions = all_transactions.sort_values(by=['Date', 'Account-ID', 'SplitID'], ascending=[False, True, True])\n",
        "\n",
        "# Formating\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].round(2)\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].apply(lambda x: \"${:,.2f}\".format(x))\n"
      ],
      "metadata": {
        "id": "EJTopk0eo8Ec"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output file generation"
      ],
      "metadata": {
        "id": "tbGNVP1OrelH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"output.csv\"): os.remove(\"output.csv\")\n",
        "all_transactions.to_csv(\"output.csv\", index=False)"
      ],
      "metadata": {
        "id": "glF2a7uTBl9T"
      },
      "execution_count": 434,
      "outputs": []
    }
  ]
}