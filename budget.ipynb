{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gyo8C4G_Qtqp_YTsSNLELTOoaZDs0SF2",
      "authorship_tag": "ABX9TyMo1VgilcWGtVk+TDE4EXGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "* This project is meant to bring all my personal finance related transactions into one easy to understand view.\n",
        "* Scope / Value descriptoon\n",
        "    * ...\n",
        "* Mechanism\n",
        "    * It reads CSV files from several US and German banks and Credit Card processors and harmonizes them into one dataframe.\n",
        "    * It maps fields such as descriptions into payees\n",
        "        * Lookup mechanism (direct hit and prefix hit) against a payee config JSON file\n",
        "        * Matching against similarity vectors per payee to identify candidates (manual adjustment of payee JSON afterwards)\n",
        "    * It categorizes each transaction or splits it into several categories\n",
        "        * by payee\n",
        "        * by pre-determination of a percentage split (e.g. for Walgreens that should be sufficient, given that I have categorized transactions since 2014)\n",
        "        * by semi-automatic order-item review split (e.g. for Apple or Amazon transactions where these files exist and where a split between utility and subscription or grocery, household products or general shopping is of interest)\n",
        "    * It works with a set of indicator field to mark aspects of interest\n",
        "        * Indicator for transactions in which automatic determinations have been taken place\n",
        "        * Task field to address open tasks\n",
        "        * ..."
      ],
      "metadata": {
        "id": "jp_J4Oz5euR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jDWIT_CWmLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paths"
      ],
      "metadata": {
        "id": "l1dgHOE5Tjap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u8iiJThN_pc"
      },
      "outputs": [],
      "source": [
        "# Path settings\n",
        "HOME_PATH = \"/content/drive/MyDrive/Colab Notebooks/budget/\"\n",
        "CONFIG_PATH = HOME_PATH + \"config/\"\n",
        "TRANSACTIONS_PATH = HOME_PATH + \"transactions/\"\n",
        "ORDERS_PATH = HOME_PATH + \"orders/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of Libraries"
      ],
      "metadata": {
        "id": "YFUcNTnFoxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "754FWtrPPMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define global Variables"
      ],
      "metadata": {
        "id": "MQ1-J3IcotCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transactions dataframe and load the JSON configuration for the different banks\n",
        "bank_transactions = {}\n",
        "\n",
        "bank_files = {\n",
        "        \"Chase\": \"chase.csv\",\n",
        "        \"Wells Fargo Checking\": \"wellsfargo_checking.csv\",\n",
        "        \"Apple\": \"apple.csv\",\n",
        "        \"Commerzbank\": \"commerzbank.csv\"\n",
        "    }\n",
        "\n",
        "config_files = {\n",
        "    \"Payee Matching\" : \"payee_matching.json\",\n",
        "    \"Exchange Rates EUR USD\": 'eur_usd_exchange_rates.csv',\n",
        "    \"Amazon Item Categories\": 'amazon_item_categories.csv'\n",
        "}\n",
        "\n",
        "# If we are using Google Drive, prefix each value in the dictionary with the ..._PATH variable\n",
        "bank_files = {key: f\"{TRANSACTIONS_PATH}{value}\" for key, value in bank_files.items()}\n",
        "config_files = {key: f\"{CONFIG_PATH}{value}\" for key, value in config_files.items()}\n",
        "\n",
        "# Define overall transactions dataframe\n",
        "all_transactions = []"
      ],
      "metadata": {
        "id": "8W7j8YnVNfEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Conversion"
      ],
      "metadata": {
        "id": "DonELYoNmHRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For each bank file:\n",
        "    * Load file into individual df\n",
        "    * Basic quality control on the individual df level\n",
        "    * Transform columns into target columns\n",
        "        * Add Bank ID field as well as numberical ID field\n",
        "    * Add individual df to transactions df\n",
        "\n",
        "* Special transformations for non-US banks:\n",
        "    * Date conversion\n",
        "    * EUR to USD conversion based on an existing file (date and exchange rate or an API call to a free service)"
      ],
      "metadata": {
        "id": "sV_EgVu5pG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quality_control(df):\n",
        "    missing_values = df.isnull().sum()\n",
        "    column_data_types = df.dtypes\n",
        "\n",
        "    return missing_values, column_data_types"
      ],
      "metadata": {
        "id": "we4_xBVmKZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_field_names(df, bank=\"\"):\n",
        "\n",
        "    if 'Category' in df.columns:\n",
        "        df = df.rename(columns={\"Category\" : \"oldCategory\"})\n",
        "\n",
        "    df.insert(4, 'SplitID',\"\")\n",
        "    df.insert(0, 'Date','')\n",
        "    df.insert(1, 'Payee','')\n",
        "    df.insert(2, 'Category Type','')\n",
        "    df.insert(3, 'Category','')\n",
        "    df.insert(4, 'chkPayee','')\n",
        "    df.insert(5, 'chkCategory','')\n",
        "    df.insert(6, 'chkSplit','')\n",
        "    df.insert(7, 'chkEURUSD','')\n",
        "\n",
        "#    if bank == \"Commerzbank\":\n",
        "#        df.insert(\"Amount (USD)\")\n",
        "#        df = df.rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sx688qkjJyLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wells Fargo"
      ],
      "metadata": {
        "id": "iSkK3j8oYDrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wells Fargo Checking"
      ],
      "metadata": {
        "id": "OKPVENVyYGzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Wells Fargo Checking CSV\n",
        "bank = 'Wells Fargo Checking'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank], header=None, names=[\"Transaction Date\", \"Amount (USD)\", \"Status\", \"Memo\", \"Description\"])\n",
        "\n",
        "# Adjust field names (if any specific adjustments are required)\n",
        "\n",
        "# Convert 'Transaction Date' column to datetime\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "\n",
        "# Check for problematic dates (rows where the date conversion failed)\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "\n",
        "# Perform quality control checks\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "# Final touches for Wells Fargo only\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction Date', 'Status', 'Memo'])  # Assuming 'Status' is not needed, adjust as necessary\n",
        "bank_transactions[bank]['Amount (USD)'] *= -1"
      ],
      "metadata": {
        "id": "3ayfeFcIJeLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chase"
      ],
      "metadata": {
        "id": "uGfOhoFalRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Chase'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Post Date', 'oldCategory', 'Type', 'Memo', 'Transaction Date'])\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Amount\" : \"Amount (USD)\"})\n"
      ],
      "metadata": {
        "id": "S2SUWsyGYA0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple"
      ],
      "metadata": {
        "id": "_PiuqQMNlTrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Apple'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank], bank)\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction Date'], errors='coerce')\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "# Final touches for Apple Card only\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction Date', 'Clearing Date', 'Merchant', 'oldCategory', 'Type', 'Purchased By'])\n",
        "bank_transactions[bank]['Amount (USD)'] *= -1\n",
        "\n",
        "#bank_transactions[bank]"
      ],
      "metadata": {
        "id": "Fb8FLo8XeVqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commerzbank"
      ],
      "metadata": {
        "id": "N8bnZFhBlU8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = 'Commerzbank'\n",
        "bank_transactions[bank] = pd.read_csv(bank_files[bank])\n",
        "\n",
        "bank_transactions[bank] = adjust_field_names(bank_transactions[bank])\n",
        "\n",
        "\n",
        "bank_transactions[bank]['Date'] = pd.to_datetime(bank_transactions[bank]['Transaction date'], errors='coerce', format='%d.%m.%Y') # For Commerzbank, Day.Month.Year\n",
        "problematic_dates = bank_transactions[bank][bank_transactions[bank]['Date'].isna()]\n",
        "missing_values, column_data_types = quality_control(bank_transactions[bank])\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank][bank_transactions[bank]['Amount'] != 0]\n",
        "\n",
        "\n",
        "bank_transactions[bank].insert(7, \"Amount (USD)\",\"\")\n",
        "\n",
        "bank_transactions[bank] = bank_transactions[bank].rename(columns={\"Booking text\" : \"Description\"})\n",
        "\n",
        "### EUR to USD conversion\n",
        "# https://www.wsj.com/market-data/quotes/fx/EURUSD/historical-prices\n",
        "\n",
        "exchange_rates_data = pd.read_csv(config_files['Exchange Rates EUR USD'])\n",
        "\n",
        "# Convert the date columns to consistent datetime format\n",
        "exchange_rates_data['Date'] = pd.to_datetime(exchange_rates_data['Date'], format='%m/%d/%Y')\n",
        "\n",
        "# Merge on the date columns to add the exchange rate to bank_transactions[bank]\n",
        "bank_transactions[bank] = bank_transactions[bank].merge(exchange_rates_data[['Date', ' Close']], on='Date', how='left')\n",
        "\n",
        "# Add the chkEURUSD column based on the ' Close' column value\n",
        "bank_transactions[bank]['chkEURUSD'] = np.where(bank_transactions[bank][' Close'].isna(), 'E', 'A')\n",
        "\n",
        "# Convert the Amount from EUR to USD\n",
        "bank_transactions[bank]['Amount (USD)'] = bank_transactions[bank]['Amount'] * bank_transactions[bank][' Close']\n",
        "\n",
        "# Drop the ' Close' column as it's not needed anymore in bank_transactions[bank]\n",
        "bank_transactions[bank].drop(' Close', axis=1, inplace=True)\n",
        "\n",
        "### End of currency conversion\n",
        "\n",
        "#bank_transactions[bank].drop(bank_transactions[bank].columns[[16, 15, 14, 13, 12, 10, 9, 8]], axis=1, inplace=True)\n",
        "bank_transactions[bank] = bank_transactions[bank].drop(columns=['Transaction date', 'Value date', 'Transaction type', 'Amount', 'Account of initiator', 'Bank code of account of initiator', 'IBAN of account of initiator'])\n"
      ],
      "metadata": {
        "id": "qLtMUpw7e1Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_transactions = pd.concat(bank_transactions, keys=bank_transactions.keys())\n",
        "all_transactions['Account-ID'] = all_transactions.index.get_level_values(0) + \"-\" + all_transactions.index.get_level_values(1).astype(str)"
      ],
      "metadata": {
        "id": "W4NvjMwc3FRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Payees"
      ],
      "metadata": {
        "id": "qvAQCjR02_0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Payee Harmonization"
      ],
      "metadata": {
        "id": "IQ2XK_tVq09c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "class MerchantMatcher:\n",
        "    def __init__(self, data_df):\n",
        "        self.data = data_df\n",
        "        self.vectorizer = self._train_vectorizer()\n",
        "        self.payee_vectors = self._compute_payee_vectors()\n",
        "        self.positive_list_descriptions = self._get_positive_list_descriptions()\n",
        "\n",
        "    def _match_prefix(self, description, merchant_details):\n",
        "        prefix_length = merchant_details.get('Prefix Length', 50)\n",
        "        for known_description in merchant_details['Positive List']:\n",
        "            truncated_payee = known_description.lower()[:prefix_length]\n",
        "            if description.lower().startswith(truncated_payee):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def _train_vectorizer(self):\n",
        "        all_descriptions = [desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions]\n",
        "        return TfidfVectorizer().fit(all_descriptions)\n",
        "\n",
        "    def _compute_payee_vectors(self):\n",
        "        payee_vectors = {}\n",
        "        for merchant, details in self.data.iterrows():\n",
        "            tfidf_matrix = self.vectorizer.transform([desc.lower() for desc in details['Positive List']])\n",
        "            avg_vector = np.asarray(tfidf_matrix.mean(axis=0))\n",
        "            payee_vectors[merchant] = avg_vector\n",
        "        return payee_vectors\n",
        "\n",
        "    def _get_positive_list_descriptions(self):\n",
        "        return set(desc.lower() for descriptions in self.data['Positive List'] for desc in descriptions)\n",
        "\n",
        "    def predict_payees(self, transaction_df):\n",
        "        mg_values = []\n",
        "        chkpayee_values = []\n",
        "        candidates = []\n",
        "\n",
        "        for _, row in transaction_df.iterrows():\n",
        "            description_lower = row['Description'].lower() if row['Description'] else None\n",
        "            current_merchant = None\n",
        "            current_chkpayee = None\n",
        "\n",
        "            if pd.isna(description_lower) or not description_lower.strip():\n",
        "                mg_values.append(None)\n",
        "                chkpayee_values.append(None)\n",
        "                continue\n",
        "\n",
        "            for merchant, details in self.data.iterrows():\n",
        "                if description_lower in [desc.lower() for desc in details['Positive List']]:\n",
        "                    current_merchant = merchant\n",
        "                    current_chkpayee = 'A'\n",
        "                    break\n",
        "\n",
        "                # Check for prefix matching\n",
        "                if self._match_prefix(description_lower, details):\n",
        "                    current_merchant = merchant\n",
        "                    current_chkpayee = 'P'\n",
        "                    break\n",
        "\n",
        "            if not current_merchant:\n",
        "                description_vector = self.vectorizer.transform([description_lower])\n",
        "                similarities = {merchant: linear_kernel(description_vector, np.asarray(vector))[0][0] for merchant, vector in self.payee_vectors.items()}\n",
        "                predicted_merchant = max(similarities, key=similarities.get)\n",
        "                max_similarity = similarities[predicted_merchant]\n",
        "\n",
        "                if max_similarity > self.data.loc[predicted_merchant, 'Threshold']:\n",
        "                    candidates.append({'Payee': predicted_merchant, 'Description': row['Description'], 'Probability': max_similarity})\n",
        "\n",
        "            mg_values.append(current_merchant)\n",
        "            chkpayee_values.append(current_chkpayee or 'C')\n",
        "\n",
        "        transaction_df['Payee'] = mg_values\n",
        "        transaction_df['chkPayee'] = chkpayee_values\n",
        "        candidates_df = pd.DataFrame(candidates)\n",
        "        return transaction_df, candidates_df\n",
        "\n",
        "\n",
        "\n",
        "data_df = pd.read_json(config_files[\"Payee Matching\"], orient=\"index\")\n",
        "\n",
        "matcher = MerchantMatcher(data_df)\n",
        "payees_identified_df, payees_candidates_df = matcher.predict_payees(all_transactions)\n",
        "payees_identified_df = payees_identified_df[payees_identified_df['chkPayee'] != 'C']\n",
        "\n",
        "file_payees_identified = \"z_payees_identified.csv\"\n",
        "file_payees_candidates = \"z_payees_candidates.csv\"\n",
        "\n",
        "if os.path.exists(file_payees_identified): os.remove(file_payees_identified)\n",
        "if os.path.exists(file_payees_candidates): os.remove(file_payees_candidates)\n",
        "payees_identified_df.to_csv(file_payees_identified, index=False)\n",
        "payees_candidates_df.to_csv(file_payees_candidates, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_Ltgm3iSlk"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categories"
      ],
      "metadata": {
        "id": "DF0bxKmWmsz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Direct assignment Transactions <--> Payee mapping (1:1)\n",
        "* Transactions <--> Amazon Orders mapping and splitting\n",
        "* Transactions <--> Apple Orders mapping and splitting\n",
        "* Transactions <--> Walgreens splitting"
      ],
      "metadata": {
        "id": "ol1JCGCXmkqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct assignment"
      ],
      "metadata": {
        "id": "u4CttrmftPPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transactions <--> Payee mapping (1:1)\n",
        "\n",
        "with open(config_files['Payee Matching'], 'r') as file:\n",
        "    payee_data = json.load(file)\n",
        "\n",
        "# List to hold split transactions\n",
        "split_transactions = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for idx, row in all_transactions.iterrows():\n",
        "    payee = row['Payee']\n",
        "\n",
        "    # Check if payee exists in the JSON data\n",
        "    if payee in payee_data:\n",
        "        categories = payee_data[payee]['Categories']\n",
        "\n",
        "        # If no category exists, update the row's category columns\n",
        "        if len(categories) == 0:\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'E'\n",
        "\n",
        "        # If only one category exists, update the row's category columns\n",
        "        if len(categories) == 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = categories[0]['Category Type']\n",
        "            all_transactions.at[idx, 'Category'] = categories[0]['Category']\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "        # If multiple categories exist, create split transactions\n",
        "        elif len(categories) > 1:\n",
        "            all_transactions.at[idx, 'Category Type'] = ''  # Empty the master row's category columns\n",
        "            all_transactions.at[idx, 'Category'] = ''\n",
        "            all_transactions.at[idx, 'SplitID'] = str(row['Account-ID']) + '-' + 'M'\n",
        "            all_transactions.at[idx, 'chkCategory'] = 'A'\n",
        "\n",
        "            for idx_split, category in enumerate(categories, start=1):\n",
        "                new_row = row.copy()\n",
        "                new_row['Category Type'] = category['Category Type']\n",
        "                new_row['Category'] = category['Category']\n",
        "                new_row['SplitID'] = str(row['Account-ID']) + '-' + 'S' + str(idx_split-1)\n",
        "                new_row['chkCategory'] = 'A'\n",
        "\n",
        "                # Update the 'Amount (USD)' based on the percentage split from the JSON\n",
        "                new_row['Amount (USD)'] = row['Amount (USD)'] * category.get('Percentage', 1)\n",
        "\n",
        "                split_transactions.append(new_row)\n",
        "\n",
        "# Append the split transactions to the main dataframe\n",
        "all_transactions = pd.concat([all_transactions, pd.DataFrame(split_transactions)], ignore_index=False)"
      ],
      "metadata": {
        "id": "s4LpNZWV_dgB"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amazon categorization\n",
        "\n",
        "1. **Identification of Amazon Transactions**:\n",
        "* Filter transactions with the Payee set to \"Amazon\" or \"Amazon Grocery\".\n",
        "* From this subset, take those transactions that don't already have a chkCategory flag.\n",
        "   \n",
        "2. **Match the Transactions to Orders**:\n",
        "   - For each identified Amazon transaction, we need to match it with an order from the Amazon order file. This matching will be based on the transaction date (with a tolerance of a few days) and the payment amount.\n",
        "   \n",
        "3. **Extract Items for the Matched Orders**:\n",
        "   - Once we have identified the matching order, we will then look up the items related to that order from the Amazon order items file.\n",
        "   \n",
        "4. **Categorize the Items**:\n",
        "   - We will categorize the items into two groups:\n",
        "     - Groceries (Split 1)\n",
        "     - All other line items (Split 2...n)\n",
        "   \n",
        "5. **Modify the Transactions**:\n",
        "   - We will then modify the transactions to reflect these splits, updating the description for each split with the appropriate line item description."
      ],
      "metadata": {
        "id": "tiI3oyFtzzf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Split and Identify Payments\n",
        "1.1. Split the payments up from the Amazon header data.\n",
        "1.2. Identify and match the transactions associated with these individual payments.\n",
        "1.3. Retire the identified transactions and replace them with a new, merged transaction that sums up these payments. This merged transaction will have the chkmerged marker and the associated order ID.\n",
        "\n",
        "Step 2: Categorize Items\n",
        "2.1. For each item in an order, identify its category as we've done before.\n",
        "\n",
        "Step 3: Compute Total for Each Order\n",
        "3.1. For each item, calculate the sum of the product of quantity and price. If quantity is NaN, assume it to be 1.\n",
        "3.2. Add taxes and shipping, and subtract the gift card amount. The result is the computed total for the order.\n",
        "3.3. Compare this computed total with the calculated overall payment for the order from the merged transaction. These two values should match.\n",
        "\n",
        "Step 4: Create Splits for Each Item\n",
        "4.1. Make the merged transaction the master transaction.\n",
        "4.2. Associate each item as a split transaction, where the split amount is the item price plus a portion of taxes, shipping, etc.\n",
        "4.3. The splits should add up to match the master transaction amount.\n",
        "\n",
        "Step 5: Roll-Up Splits by Category\n",
        "5.1. Group the splits by their category for each order.\n",
        "5.2. Sum up the amounts within these groups.\n",
        "5.3. Retain only one split per category, with the summed amount.\n",
        "5.4. These rolled-up splits should still add up to match the master transaction amount."
      ],
      "metadata": {
        "id": "UXr0bRUiJKIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going forward plan\n",
        "\n",
        "Thank you for the comprehensive code. Let's go through it step by step and re-engineer based on our new plan:\n",
        "\n",
        "1. AmazonProcessor (Cell 1)\n",
        "This class is responsible for processing the Amazon order headers, particularly to split multiple payments.\n",
        "\n",
        "**Usefulness**: This class is essential. We need to split multiple payments to associate them with individual transactions.\n",
        "\n",
        "2. AmazonTransactionMatcher (Cell 2)\n",
        "This class matches the processed Amazon payments (from the AmazonProcessor) with the actual bank transactions.\n",
        "\n",
        "**Usefulness**: This class is crucial. After processing the Amazon headers, we need to match them to actual transactions.\n",
        "\n",
        "3. CategoryIdentification (Cell 3)\n",
        "This class categorizes each item based on a keyword list.\n",
        "\n",
        "**Usefulness**: Still essential. We want to identify each item's category.\n",
        "\n",
        "4. AmazonProcessor (Cell 4)\n",
        "This class processes the merged DataFrame (`final_df`) to handle grocery and non-grocery splits and integrate them into the `all_transactions`.\n",
        "\n",
        "**Usefulness**: This needs restructuring. The logic here is to be modified as per our new approach.\n",
        "\n",
        "---\n",
        "\n",
        "Proposed Structure:\n",
        "1. **AmazonPaymentProcessor**: Refactor `AmazonProcessor` from Cell 1. This will process the Amazon headers and split multiple payments.\n",
        "   \n",
        "2. **TransactionMatcher**: Refactor `AmazonTransactionMatcher` from Cell 2. This will match Amazon payments to actual transactions. After matching, create a single merged transaction for each order, marking it with `chkmerged`.\n",
        "\n",
        "3. **ItemCategorizer**: Refactor `CategoryIdentification` from Cell 3. This class will assign categories to items.\n",
        "\n",
        "4. **TransactionUpdater**: Create a new class (replacing `AmazonProcessor` from Cell 4). This class will:\n",
        "    - Calculate the total for each order.\n",
        "    - Check the totals against the merged transaction values.\n",
        "    - Create splits for each item.\n",
        "    - Roll up splits by category.\n",
        "\n",
        "---\n",
        "\n",
        "Steps for Implementation:\n",
        "\n",
        "1. Rename and restructure `AmazonProcessor` from Cell 1 to `AmazonPaymentProcessor`.\n",
        "   \n",
        "2. Rename and restructure `AmazonTransactionMatcher` from Cell 2 to `TransactionMatcher`. Modify it to create the merged transactions.\n",
        "\n",
        "3. Rename and slightly modify `CategoryIdentification` from Cell 3 to `ItemCategorizer`.\n",
        "\n",
        "4. Replace `AmazonProcessor` from Cell 4 with a new class named `TransactionUpdater`. Modify the logic as per our new approach.\n",
        "\n",
        "Would you like to proceed with this structure?"
      ],
      "metadata": {
        "id": "8utsf0lWKBPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon Payment Processor"
      ],
      "metadata": {
        "id": "VO10hFL8P3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_order_headers = pd.read_csv(ORDERS_PATH + \"amazon_headers_combined.csv\", parse_dates=['Order Date', 'Payment Date'])"
      ],
      "metadata": {
        "id": "G05g5zt07HT0"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon Transaction Matcher"
      ],
      "metadata": {
        "id": "pxxoICcdP8n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonDataMatcher:\n",
        "    def __init__(self, headers_df, transactions_df):\n",
        "        self.headers_df = headers_df\n",
        "        self.transactions_df = transactions_df\n",
        "        self.matched_df = None\n",
        "\n",
        "    def match_records(self):\n",
        "        # Filter transactions based on Payee criteria\n",
        "        valid_payees = [\"Amazon\", \"Amazon Grocery\", \"Amazon Prime\"]\n",
        "        filtered_transactions = self.transactions_df[self.transactions_df['Payee'].isin(valid_payees)]\n",
        "\n",
        "        # Sort DataFrames for merge_asof\n",
        "        self.headers_df = self.headers_df.sort_values('Payment Date')\n",
        "        filtered_transactions = filtered_transactions.sort_values('Date')\n",
        "\n",
        "        # Basic merge on the closest date. This doesn't consider the +/- 2 days condition yet.\n",
        "        merged_df = pd.merge_asof(self.headers_df, filtered_transactions,\n",
        "                                left_on='Payment Date',\n",
        "                                right_on='Date',\n",
        "                                direction='nearest')\n",
        "\n",
        "        # Filter out rows where the date difference is more than 2 days.\n",
        "        merged_df = merged_df[abs((merged_df['Payment Date'] - merged_df['Date']).dt.days) <= 5]\n",
        "\n",
        "        # Convert 'Amount (USD)' to float\n",
        "        merged_df['Amount (USD)'] = pd.to_numeric(merged_df['Amount (USD)'], errors='coerce')\n",
        "\n",
        "        # Handle the sign difference between 'Payment Amount' and 'Amount (USD)'\n",
        "        merged_df = merged_df[(merged_df['Payment Amount'] == -merged_df['Amount (USD)']) |\n",
        "                            (merged_df['Payment Amount'] == merged_df['Amount (USD)'])]\n",
        "\n",
        "        self.matched_df = merged_df\n",
        "\n",
        "    def save_to_csv(self, output_path):\n",
        "        if self.matched_df is not None:\n",
        "            self.matched_df.to_csv(output_path, index=False)\n",
        "            print(f\"Matching complete and saved to {output_path}!\")\n",
        "        else:\n",
        "            print(\"No matched data to save.\")\n",
        "\n",
        "    def verify_match(self):\n",
        "        # Verification on the Amount (USD) level\n",
        "        matched_amounts_sum = self.matched_df['Payment Amount'].sum()\n",
        "        transactions_amounts_sum = self.transactions_df['Amount (USD)'].sum()\n",
        "\n",
        "        # Check if the sums are approximately equal, considering potential rounding errors\n",
        "        if abs(matched_amounts_sum + transactions_amounts_sum) < 0.05:\n",
        "            print(\"Verification passed: Matched amounts are consistent with the transaction amounts.\")\n",
        "        else:\n",
        "            print(\"Verification failed: There's a discrepancy in the matched amounts.\")\n",
        "\n",
        "    def update_transactions_with_matches(self, all_transactions):\n",
        "        # Left merge to get matching 'Order ID' into the all_transactions DataFrame\n",
        "        merged = all_transactions.merge(self.matched_df[['Account-ID', 'Order ID']],\n",
        "                                        on='Account-ID',\n",
        "                                        how='left')\n",
        "\n",
        "        # Update the 'Category' column with 'Order ID' where matches occurred\n",
        "        merged['Category'] = merged['Order ID'].combine_first(merged['Category'])\n",
        "\n",
        "        # Drop the 'Order ID' column as it's no longer needed\n",
        "        merged.drop('Order ID', axis=1, inplace=True)\n",
        "\n",
        "        return merged"
      ],
      "metadata": {
        "id": "f7Oz_2cRg-WS"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the class\n",
        "matcher = AmazonDataMatcher(amazon_order_headers, all_transactions)\n",
        "\n",
        "\n",
        "# Match the records\n",
        "matcher.match_records()\n",
        "\n",
        "# Verify the match\n",
        "matcher.verify_match()\n",
        "\n",
        "# Save the matched data (optional)\n",
        "matcher.save_to_csv('matched_records.csv')\n",
        "\n",
        "#all_transactions = matcher.update_transactions_with_matches(all_transactions)\n",
        "matched_transactions_with_headers = matcher.matched_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLK-YmdFhLIU",
        "outputId": "969b5e26-7dec-458f-b2a0-c43f099c0a78"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification failed: There's a discrepancy in the matched amounts.\n",
            "Matching complete and saved to matched_records.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon Categorizer"
      ],
      "metadata": {
        "id": "8Fv0aBdc37Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class CategoryIdentification:\n",
        "    def __init__(self, order_items_df, config_df, similarity_threshold=0.3):\n",
        "        self.order_items = order_items_df\n",
        "        self.config = config_df\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def identify_category(self, item_description):\n",
        "        # Check if item_description is a string\n",
        "        if not isinstance(item_description, str):\n",
        "            return None, None\n",
        "\n",
        "        # Direct match with config\n",
        "        matched_category = self.config[self.config['Item Desciption'].str.lower() == item_description.lower()]['Item Category'].values\n",
        "        if matched_category.size > 0:\n",
        "            return matched_category[0], \"A\"  # A for Automatic/Exact\n",
        "\n",
        "        # Compute vector for item description\n",
        "        vectorizer = TfidfVectorizer().fit(self.config['Item Keyword'].tolist() + [item_description])\n",
        "        item_vector = vectorizer.transform([item_description])\n",
        "\n",
        "        # Compute vectors for all keywords in config\n",
        "        keyword_vectors = vectorizer.transform(self.config['Item Keyword'])\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity(item_vector, keyword_vectors)\n",
        "\n",
        "        # Identify best match above threshold\n",
        "        best_match_index = similarities.argmax()\n",
        "        if similarities[0, best_match_index] > self.similarity_threshold:\n",
        "            return self.config.iloc[best_match_index]['Item Category'], \"S\"  # S for Similar\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def categorize_order_items(self):\n",
        "        self.order_items['Item Category'], self.order_items['chkMatch'] = zip(*self.order_items['Description'].map(self.identify_category))\n",
        "        return self.order_items\n",
        "\n",
        "    def get_uncategorized_items(self):\n",
        "        return self.order_items[self.order_items['Item Category'].isnull()]\n",
        "\n",
        "\n",
        "\n",
        "amazon_order_items = pd.read_csv(ORDERS_PATH + \"amazon_items_combined.csv\", parse_dates=['Order Date'])\n",
        "order_items_df = amazon_order_items # refactor this later\n",
        "keyword_df = pd.read_csv(config_files['Amazon Item Categories'])\n",
        "category_identifier = CategoryIdentification(order_items_df, keyword_df)\n",
        "\n",
        "categorized_order_items = category_identifier.categorize_order_items()\n",
        "\n",
        "categorized_order_items = categorized_order_items.rename(columns={'Description': 'Item Description'})\n",
        "categorized_order_items = categorized_order_items.rename(columns={'category': 'Item Category'})\n",
        "\n",
        "# Rename the description column in matched_transactions to avoid conflict\n",
        "#matched_transactions = matched_transactions.rename(columns={'description': 'Transaction Description'})\n",
        "\n",
        "# Merge the dataframes\n",
        "final_df = pd.merge(matched_transactions_with_headers, categorized_order_items[['Order ID', 'Item Description', 'Item Category', 'Price', 'Quantity', 'chkMatch']], on='Order ID', how=\"left\")\n",
        "\n",
        "print(categorized_order_items.columns)\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items, on=\"order id\", how=\"left\")\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items[['order id', 'description', 'category']], on=\"order id\", how=\"left\")\n",
        "\n",
        "uncategorized_items = category_identifier.get_uncategorized_items()\n",
        "\n",
        "final_df = final_df[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Payment Date',\n",
        "    'Payment Amount',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Description',\n",
        "    'Item Description',\n",
        "    'Item Category',\n",
        "    'chkMatch',\n",
        "    'Price',\n",
        "    'Quantity',\n",
        "    'Total',\n",
        "    'Shipping',\n",
        "    'Shipping Refund',\n",
        "    'Gift',\n",
        "    'Tax',\n",
        "    'Refund',\n",
        "    'Origin',\n",
        "    'Order ID',\n",
        "    'Currency',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "\n",
        "if os.path.exists(\"final_df.csv\"): os.remove(\"final_df.csv\")\n",
        "final_df.to_csv(\"final_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "upsTJv6_QCeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b80817-d81d-4df3-fbe4-c13a67afe919"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Origin', 'ID', 'Order ID', 'Order Date', 'Quantity',\n",
            "       'Item Description', 'Price', 'chkQuantity', 'Item Category',\n",
            "       'chkMatch'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonProcessorDebugStep2V4:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df.sort_values(by='Order ID')\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def modify_all_transactions_step2(self):\n",
        "        modified_amazon_transactions = []\n",
        "\n",
        "        # Process Amazon Transactions by grouping by Account-ID and Item Category\n",
        "        for (account_id, item_category), group in self.final_df.groupby(['Account-ID', 'Item Category']):\n",
        "            main_transaction = group.iloc[0]  # Use the first row for the main transaction details\n",
        "\n",
        "            # If Item Category exists, aggregate by category\n",
        "            if pd.notna(item_category):\n",
        "                total_amount = (group['Price'] * group['Quantity']).sum()\n",
        "                main_row = main_transaction.copy()\n",
        "                main_row['Payee'] = np.nan\n",
        "                main_row['chkSplit'] = \"SPLIT\"  # Set chkSplit to \"SPLIT\"\n",
        "                main_row['Amount (USD)'] = np.nan  # Set Amount (USD) to NaN for splits\n",
        "                main_row['Item Description'] = np.nan  # Set Amount (USD) to NaN for splits\n",
        "                modified_amazon_transactions.append(main_row.to_dict())\n",
        "\n",
        "        # Set chkSplit to \"MSTR\" for master transactions in all_transactions\n",
        "        amazon_account_ids = self.final_df['Account-ID'].unique()\n",
        "        self.all_transactions.loc[self.all_transactions['Account-ID'].isin(amazon_account_ids), 'chkSplit'] = \"MSTR\"\n",
        "\n",
        "        # Combine the modified Amazon transactions with the all_transactions dataframe\n",
        "        modified_amazon_df = pd.DataFrame(modified_amazon_transactions)\n",
        "        combined_df = pd.concat([self.all_transactions, modified_amazon_df], ignore_index=True, sort=False)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "# Debug Step 2\n",
        "processor_debug_step2_v4 = AmazonProcessorDebugStep2V4(final_df, all_transactions)\n",
        "combined_transactions_step2_v4 = processor_debug_step2_v4.modify_all_transactions_step2()\n",
        "\n",
        "combined_transactions_step2_v4 = combined_transactions_step2_v4[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'chkSplit',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Payment Date',\n",
        "    'Payment Amount',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Item Description',\n",
        "    'Item Category',\n",
        "    'Description',\n",
        "    'chkMatch',\n",
        "    'Price',\n",
        "    'Quantity',\n",
        "    'Total',\n",
        "    'Shipping',\n",
        "    'Shipping Refund',\n",
        "    'Gift',\n",
        "    'Tax',\n",
        "    'Refund',\n",
        "    'Origin',\n",
        "    'Order ID',\n",
        "    'Currency',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "# Output\n",
        "if os.path.exists(\"all_df.csv\"): os.remove(\"all_df.csv\")\n",
        "combined_transactions_step2_v4.to_csv(\"all_df.csv\", index=False)\n",
        "\n",
        "# Return a message indicating the completion\n",
        "\"Updated transactions have been saved to all_df.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RsL1CSGcqKG6",
        "outputId": "9afe1b38-922d-44b2-b468-59eb44bce073"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Updated transactions have been saved to all_df.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### later or not\n",
        "class AmazonProcessorStep123:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def process_transactions_step123(self):\n",
        "        # Step 1: Calculate SUMME for each Account-ID and Item Category\n",
        "        self.final_df['line_total'] = self.final_df['Price'] * self.final_df['Quantity']\n",
        "        category_summe = self.final_df.groupby(['Account-ID', 'Item Category'])['line_total'].sum()\n",
        "\n",
        "        # Step 2: Calculate Overall SUMME for each Order\n",
        "        order_summe = self.final_df.groupby('Account-ID')['line_total'].sum()\n",
        "\n",
        "        # Step 3: Retrieve Paid Amount\n",
        "        paid_amounts = self.all_transactions.set_index('Account-ID')['Amount (USD)']\n",
        "\n",
        "        # Joining the results for verification\n",
        "        results = pd.concat([order_summe, category_summe, paid_amounts], axis=1)\n",
        "        results.columns = ['Order SUMME', 'Category SUMME', 'Paid Amount']\n",
        "\n",
        "        return results\n",
        "\n",
        "# Debug Steps 1-3\n",
        "processor_123 = AmazonProcessorStep123(final_df, all_transactions)\n",
        "results_123 = processor_123.process_transactions_step123()\n",
        "print(results_123)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "fS5hwf8brAXg",
        "outputId": "c92f631f-f4db-4f69-a816-9462a0a72db3"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-157-4d0fee021510>:19: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
            "  results = pd.concat([order_summe, category_summe, paid_amounts], axis=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-4d0fee021510>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Debug Steps 1-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprocessor_123\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazonProcessorStep123\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_transactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresults_123\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_123\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_transactions_step123\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-4d0fee021510>\u001b[0m in \u001b[0;36mprocess_transactions_step123\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Joining the results for verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_summe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_summe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaid_amounts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Order SUMME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Category SUMME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Paid Amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# _homogenize ensures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#  - all(len(x) == len(index) for x in arrays)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;31m# Forces alignment. No need to copy data since we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;31m# are putting it into an ndarray later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5092\u001b[0m                 )\n\u001b[1;32m   5093\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5094\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5096\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5288\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5289\u001b[0;31m         return self._reindex_axes(\n\u001b[0m\u001b[1;32m   5290\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5291\u001b[0m         ).__finalize__(self, method=\"reindex\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5308\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5309\u001b[0;31m             obj = obj._reindex_with_indexers(\n\u001b[0m\u001b[1;32m   5310\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5311\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   5353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5354\u001b[0m             \u001b[0;31m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5355\u001b[0;31m             new_data = new_data.reindex_indexer(\n\u001b[0m\u001b[1;32m   5356\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_validate_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   4314\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4316\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex on an axis with duplicate labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4318\u001b[0m     def reindex(\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### backup before summe\n",
        "class AmazonProcessorRevisedV2:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df.sort_values(by='Order ID')\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def modify_all_transactions(self):\n",
        "        # Step 1: Mark Amazon-related transactions in all_transactions with chkSplit as \"MSTR\"\n",
        "        amazon_account_ids = self.final_df['Account-ID'].unique()\n",
        "        self.all_transactions.loc[self.all_transactions['Account-ID'].isin(amazon_account_ids), 'chkSplit'] = \"MSTR\"\n",
        "\n",
        "        modified_amazon_transactions = []\n",
        "\n",
        "        # Step 2: Process Amazon Transactions by grouping by Account-ID and Item Category\n",
        "        for (account_id, item_category), group in self.final_df.groupby(['Account-ID', 'Item Category']):\n",
        "            main_transaction = group.iloc[0]  # Use the first row for the main transaction details\n",
        "\n",
        "            print(account_id + \" \" + item_category)\n",
        "            # If Item Category exists, aggregate by category\n",
        "            if pd.notna(item_category):\n",
        "                total_amount = (group['Price'] * group['Quantity']).sum()\n",
        "                main_row = main_transaction.copy()\n",
        "                main_row['Payee'] = 'Split Transaction'\n",
        "#                main_row['Description'] = f'Amazon {item_category}'\n",
        "#                main_row['Amount (USD)'] = total_amount\n",
        "                main_row['chkSplit'] = \"SPLIT\"  # Set chkSplit to \"SPLIT\"\n",
        "                modified_amazon_transactions.append(main_row)\n",
        "            else:  # For items without an Item Category, create separate rows\n",
        "                for _, row in group.iterrows():\n",
        "                    item_row = row.copy()\n",
        "                    item_row['Payee'] = 'Split Transaction'\n",
        "#                    item_row['Description'] = row['Description']\n",
        "#                    item_row['Amount (USD)'] = row['Price'] * row['Quantity']\n",
        "                    item_row['chkSplit'] = \"SPLIT\"  # Set chkSplit to \"SPLIT\"\n",
        "                    modified_amazon_transactions.append(item_row)\n",
        "\n",
        "        # Step 3: Append the modified Amazon transactions back to all_transactions\n",
        "        amazon_df = pd.DataFrame(modified_amazon_transactions)\n",
        "        # Resetting indices before concatenation\n",
        "        self.all_transactions = self.all_transactions.reset_index(drop=True)\n",
        "        amazon_df = amazon_df.reset_index(drop=True)\n",
        "        self.all_transactions = pd.concat([self.all_transactions, amazon_df], ignore_index=True, sort=False)\n",
        "\n",
        "        # Dropping any potential duplicate rows\n",
        "        self.all_transactions.drop_duplicates(inplace=True)\n",
        "\n",
        "        return self.all_transactions\n",
        "\n",
        "\n",
        "# Apply the revised method\n",
        "processor_revised_v2 = AmazonProcessorRevisedV2(final_df, all_transactions)\n",
        "updated_all_transactions_revised_v2 = processor_revised_v2.modify_all_transactions()\n",
        "\n",
        "# Output\n",
        "if os.path.exists(\"all_df.csv\"): os.remove(\"all_df.csv\")\n",
        "updated_all_transactions_revised_v2.to_csv(\"all_df.csv\", index=False)\n",
        "\n",
        "# Return a message indicating the completion\n",
        "\"Updated transactions have been saved to all_df.csv\"\n"
      ],
      "metadata": {
        "id": "igECNmtn6uPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BACKUP\n",
        "\n",
        "class AmazonProcessor:\n",
        "    def __init__(self, final_df, all_transactions):\n",
        "        self.final_df = final_df.sort_values(by='Order ID')\n",
        "        self.all_transactions = all_transactions\n",
        "\n",
        "    def modify_all_transactions(self):\n",
        "        # Step 1: Remove Amazon-related transactions from the all_transactions DataFrame\n",
        "        self.all_transactions = self.all_transactions[~self.all_transactions['Payee'].str.contains('Amazon', na=False)]\n",
        "\n",
        "        modified_amazon_transactions = []\n",
        "\n",
        "        # Step 2: Process Amazon Transactions\n",
        "        for order_id, group in self.final_df.groupby('order id'):\n",
        "            main_transaction = group.iloc[0]  # Use the first row for the main transaction details\n",
        "\n",
        "            # Main transaction row: ensure it has all columns from all_transactions\n",
        "            main_row = main_transaction.to_dict()\n",
        "            modified_amazon_transactions.append(main_row)\n",
        "\n",
        "            # Splits for grocery items\n",
        "            grocery_rows = group[group['category'] == 'grocery']\n",
        "            if not grocery_rows.empty:\n",
        "                total_grocery_amount = grocery_rows['price'].sum()\n",
        "                grocery_row = main_transaction.copy()\n",
        "                grocery_row['Payee'] = 'Split Transaction'\n",
        "                grocery_row['Description'] = 'Amazon Groceries'\n",
        "                grocery_row['Amount (USD)'] = total_grocery_amount\n",
        "                modified_amazon_transactions.append(grocery_row)\n",
        "\n",
        "            # Splits for non-grocery items\n",
        "            non_grocery_rows = group[group['category'] != 'grocery']\n",
        "            for _, row in non_grocery_rows.iterrows():\n",
        "                non_grocery_row = row.copy()\n",
        "                non_grocery_row['Payee'] = 'Split Transaction'\n",
        "                non_grocery_row['Description'] = row['description']\n",
        "                non_grocery_row['Amount'] = row['price']\n",
        "                modified_amazon_transactions.append(non_grocery_row)\n",
        "\n",
        "        # Step 3: Append the modified Amazon transactions back to all_transactions\n",
        "        amazon_df = pd.DataFrame(modified_amazon_transactions)\n",
        "        self.all_transactions = pd.concat([self.all_transactions, amazon_df], ignore_index=True)\n",
        "\n",
        "        return self.all_transactions\n",
        "\n",
        "processor = AmazonProcessor(final_df, all_transactions)\n",
        "updated_all_transactions = processor.modify_all_transactions()\n",
        "\n",
        "processor = AmazonProcessor(final_df, all_transactions)\n",
        "updated_all_transactions = processor.modify_all_transactions()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(\"final_df.csv\"): os.remove(\"final_df.csv\")\n",
        "updated_all_transactions.to_csv(\"final_df.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "YfCn8JbK2iU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "auc-YLI0qPmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe preparation"
      ],
      "metadata": {
        "id": "Ojxb0cORqS37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder Columns\n",
        "\n",
        "all_transactions = all_transactions[[\n",
        "    'Date',\n",
        "    'Account-ID',\n",
        "    'SplitID',\n",
        "    'Payee',\n",
        "    'Category Type',\n",
        "    'Category',\n",
        "    'Amount (USD)',\n",
        "    'Description',\n",
        "    'chkPayee',\n",
        "    'chkCategory',\n",
        "    'chkEURUSD']]\n",
        "\n",
        "# Sort rows\n",
        "all_transactions = all_transactions.sort_values(by=['Date', 'Account-ID', 'SplitID'], ascending=[False, True, True])\n",
        "\n",
        "# Formating\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].round(2)\n",
        "all_transactions['Amount (USD)'] = all_transactions['Amount (USD)'].apply(lambda x: \"${:,.2f}\".format(x))\n"
      ],
      "metadata": {
        "id": "EJTopk0eo8Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output file generation"
      ],
      "metadata": {
        "id": "tbGNVP1OrelH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"z_output.csv\"): os.remove(\"z_output.csv\")\n",
        "all_transactions.to_csv(\"z_output.csv\", index=False)"
      ],
      "metadata": {
        "id": "glF2a7uTBl9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}