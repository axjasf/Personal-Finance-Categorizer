{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1UWYqCPeo-X2d_ZqNEtMLT229QpY7jXoC",
      "authorship_tag": "ABX9TyPtsf7CtT6zOEbcNz9SPgra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/Combine_Amazon_Files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "6e7VnU01YrUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3u8iiJThN_pc"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Path settings\n",
        "HOME_PATH = \"/content/drive/MyDrive/Colab Notebooks/budget/\"\n",
        "CONFIG_PATH = HOME_PATH + \"config/\"\n",
        "TRANSACTIONS_PATH = HOME_PATH + \"transactions/\"\n",
        "ORDERS_PATH = HOME_PATH + \"orders/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the filenames along with their respective prefixes in a dictionary\n",
        "file_dict = {\n",
        "    \"header_files\": [\n",
        "        {\"filename\": \"amazon_order_headers_axel.csv\", \"origin\": \"A\"},\n",
        "        {\"filename\": \"amazon_order_headers_danielle.csv\", \"origin\": \"D\"}\n",
        "    ],\n",
        "    \"item_files\": [\n",
        "        {\"filename\": \"amazon_order_items_axel.csv\", \"origin\": \"A\"},\n",
        "        {\"filename\": \"amazon_order_items_danielle.csv\", \"origin\": \"D\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_files = {\n",
        "    \"Amazon Item Categories\": 'amazon_item_categories.csv'\n",
        "}\n",
        "\n",
        "config_files = {key: f\"{CONFIG_PATH}{value}\" for key, value in config_files.items()}"
      ],
      "metadata": {
        "id": "Cl2oedNw-QEr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine files\n"
      ],
      "metadata": {
        "id": "B0eMAjB-TKqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonOrderFilesProcessor:\n",
        "    def __init__(self, base_path, file_dict):\n",
        "        self.base_path = base_path\n",
        "        self.file_dict = file_dict\n",
        "        self.combined_header_df = None\n",
        "        self.combined_item_df = None\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_date_format(date_str):\n",
        "        if not isinstance(date_str, str):  # Add a check for non-string values\n",
        "            print(f\"Warning: Encountered non-string date value: {date_str}\")\n",
        "            return date_str\n",
        "\n",
        "        pattern = r'(\\d{1,2})\\s*(\\d{1,2})\\s*(\\d{4})'\n",
        "        match = re.match(pattern, date_str)\n",
        "        if match:\n",
        "            month, day, year = match.groups()\n",
        "            return f\"{year}-{int(month):02d}-{int(day):02d}\"\n",
        "        else:\n",
        "            return date_str\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_multiple_payments(payment_string):\n",
        "        if not isinstance(payment_string, str):\n",
        "            return []\n",
        "\n",
        "        pattern = r\"(?:MasterCard|Visa)?(?:\\s*ending\\s*in\\s*\\d{4})?:?\\s*([A-Za-z]*\\s*\\d{1,2},?\\s*\\d{4}|\\d{4}-\\d{1,2}-\\d{1,2})[^$]*\\$\\s*([\\d,]+\\.\\d{2})\"\n",
        "        matches = re.findall(pattern, payment_string)\n",
        "\n",
        "        month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "        month_map = {month: str(index + 1) for index, month in enumerate(month_names)}\n",
        "\n",
        "        processed_matches = []\n",
        "        for date, amount in matches:\n",
        "            for month, month_num in month_map.items():\n",
        "                date = date.replace(month, month_num)\n",
        "            date = date.replace(\",\", \"\").replace(\" \", \"-\")\n",
        "            date = AmazonOrderFilesProcessor.normalize_date_format(date)  # normalize the date format here\n",
        "            processed_matches.append((date, float(amount.replace(',', ''))))\n",
        "\n",
        "        return processed_matches\n",
        "\n",
        "    def split_multiple_payments(self, df):\n",
        "        new_rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            payments = self.extract_multiple_payments(row['Payments'])\n",
        "\n",
        "            for date, amount in payments:\n",
        "                new_row = row.copy()\n",
        "                new_row['Payment Date'] = pd.to_datetime(date)\n",
        "                new_row['Payment Amount'] = amount\n",
        "                new_rows.append(new_row)\n",
        "\n",
        "        return pd.DataFrame(new_rows)\n",
        "\n",
        "    def process_header_file(self, file_info):\n",
        "        df = pd.read_csv(self.base_path + file_info[\"filename\"])\n",
        "\n",
        "        # Standardize column spelling and drop unnecessary columns\n",
        "        df.columns = df.columns.str.capitalize()\n",
        "        df = df.rename(columns={'Order id': 'Order ID', 'Shipping_refund': 'Shipping Refund', 'Date': 'Order Date'})\n",
        "        df = df.drop(columns=['Items', 'To'])\n",
        "\n",
        "        # Remove header rows\n",
        "        df = df[(df['Order Date'] != 'date') & (df['Payments'] != 'payments')]\n",
        "\n",
        "        # Remove pending rows\n",
        "        df = df[(df['Total'] != 'pending')]\n",
        "\n",
        "        # Normalize the 'Date' column and turn it into a Date object\n",
        "        df['Order Date'] = df['Order Date'].apply(self.normalize_date_format)\n",
        "        df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "        # Add Origin and ID\n",
        "        df[\"Origin\"] = file_info[\"origin\"]\n",
        "        df[\"ID\"] = range(1, len(df) + 1)\n",
        "\n",
        "        return self.split_multiple_payments(df)\n",
        "\n",
        "    def process_item_file(self, file_info):\n",
        "        df = pd.read_csv(self.base_path + file_info[\"filename\"])\n",
        "\n",
        "        # Standardize column spelling\n",
        "        df.columns = df.columns.str.capitalize()\n",
        "        df = df.rename(columns={'Order id': 'Order ID', 'Order date': 'Order Date'})\n",
        "\n",
        "        # Remove header rows\n",
        "        df = df[(df['Price'] != 'price')]\n",
        "\n",
        "        # Normalize the 'Date' column and turn it into a Date object\n",
        "        df['Order Date'] = df['Order Date'].apply(self.normalize_date_format)\n",
        "        df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "        # Check for empty or NaN values in the 'Quantity' column and update accordingly\n",
        "        is_empty_quantity = df['Quantity'].isnull() | (df['Quantity'] == '')\n",
        "        df.loc[is_empty_quantity, 'Quantity'] = 1\n",
        "        df['chkQuantity'] = 'N'\n",
        "        df.loc[is_empty_quantity, 'chkQuantity'] = 'Y'\n",
        "\n",
        "        # Remove $ sign and format as float\n",
        "        df['Price'] = df['Price'].str.replace('$', '', regex=True).astype(float)\n",
        "\n",
        "        # Add Origin and ID\n",
        "        df[\"Origin\"] = file_info[\"origin\"]\n",
        "        df[\"ID\"] = range(1, len(df) + 1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def process_files(self):\n",
        "        header_dfs = [self.process_header_file(file_info) for file_info in self.file_dict[\"header_files\"]]\n",
        "        self.combined_header_df = pd.concat(header_dfs, ignore_index=True)\n",
        "\n",
        "        # Rearrange columns for header df\n",
        "        cols = ['Origin', 'ID'] + [col for col in self.combined_header_df if col not in ['Origin', 'ID']]\n",
        "        self.combined_header_df = self.combined_header_df[cols]\n",
        "\n",
        "        item_dfs = [self.process_item_file(file_info) for file_info in self.file_dict[\"item_files\"]]\n",
        "        self.combined_item_df = pd.concat(item_dfs, ignore_index=True)\n",
        "\n",
        "        # Rearrange columns for item df\n",
        "        cols = ['Origin', 'ID'] + [col for col in self.combined_item_df if col not in ['Origin', 'ID']]\n",
        "        self.combined_item_df = self.combined_item_df[cols]\n",
        "\n",
        "        return self.combined_header_df, self.combined_item_df\n",
        "\n",
        "# Create an instance of the class and process the files\n",
        "processor = AmazonOrderFilesProcessor(ORDERS_PATH, file_dict)\n",
        "combined_headers_df, combined_items_df = processor.process_files()"
      ],
      "metadata": {
        "id": "-M1r5d30Au8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d276af2-8e20-4fb4-d5d9-e78d73cd166c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Encountered non-string date value: nan\n",
            "Warning: Encountered non-string date value: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorize items\n"
      ],
      "metadata": {
        "id": "Jlt3iCsq7wMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class CategoryIdentification:\n",
        "    def __init__(self, order_items_df, config_df, similarity_threshold=0.3):\n",
        "        self.order_items = order_items_df\n",
        "        self.config = config_df\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def identify_category(self, item_description):\n",
        "        # Check if item_description is a string\n",
        "        if not isinstance(item_description, str):\n",
        "            return None, None\n",
        "\n",
        "        # Direct match with config\n",
        "        matched_category = self.config[self.config['Item Desciption'].str.lower() == item_description.lower()]['Item Category'].values\n",
        "        if matched_category.size > 0:\n",
        "            return matched_category[0], \"A\"  # A for Automatic/Exact\n",
        "\n",
        "        # Compute vector for item description\n",
        "        vectorizer = TfidfVectorizer().fit(self.config['Item Keyword'].tolist() + [item_description])\n",
        "        item_vector = vectorizer.transform([item_description])\n",
        "\n",
        "        # Compute vectors for all keywords in config\n",
        "        keyword_vectors = vectorizer.transform(self.config['Item Keyword'])\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity(item_vector, keyword_vectors)\n",
        "\n",
        "        # Identify best match above threshold\n",
        "        best_match_index = similarities.argmax()\n",
        "        if similarities[0, best_match_index] > self.similarity_threshold:\n",
        "            return self.config.iloc[best_match_index]['Item Category'], \"S\"  # S for Similar\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def categorize_order_items(self):\n",
        "        self.order_items['Item Category'], self.order_items['chkMatch'] = zip(*self.order_items['Description'].map(self.identify_category))\n",
        "        return self.order_items\n",
        "\n",
        "    def get_uncategorized_items(self):\n",
        "        return self.order_items[self.order_items['Item Category'].isnull()]\n",
        "\n",
        "\n",
        "order_items_df = combined_items_df\n",
        "keyword_df = pd.read_csv(config_files['Amazon Item Categories'])\n",
        "category_identifier = CategoryIdentification(order_items_df, keyword_df)\n",
        "\n",
        "categorized_order_items = category_identifier.categorize_order_items()\n",
        "\n",
        "categorized_order_items = categorized_order_items.rename(columns={'Description': 'Item Description'})\n",
        "categorized_order_items = categorized_order_items.rename(columns={'category': 'Item Category'})\n",
        "\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items, on=\"order id\", how=\"left\")\n",
        "#final_df = pd.merge(matched_transactions, categorized_order_items[['order id', 'description', 'category']], on=\"order id\", how=\"left\")\n",
        "\n",
        "combined_items_df = order_items_df\n",
        "uncategorized_items = category_identifier.get_uncategorized_items()"
      ],
      "metadata": {
        "id": "upsTJv6_QCeA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Combined Files"
      ],
      "metadata": {
        "id": "UuzSmhK_Y2IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export both files into Google Drive\n",
        "combined_headers_df.to_csv(f\"{ORDERS_PATH}amazon_headers_combined.csv\", index=False)\n",
        "combined_items_df.to_csv(f\"{ORDERS_PATH}amazon_items_combined.csv\", index=False)"
      ],
      "metadata": {
        "id": "LjmSgB-na5JN"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}