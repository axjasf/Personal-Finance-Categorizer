{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UWYqCPeo-X2d_ZqNEtMLT229QpY7jXoC",
      "authorship_tag": "ABX9TyNhO30Huigmwao/SG7njJbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axjasf/YNAB-Categorizer/blob/main/Combine_Amazon_Files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "6e7VnU01YrUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3u8iiJThN_pc"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Path settings\n",
        "HOME_PATH = \"/content/drive/MyDrive/Colab Notebooks/budget/\"\n",
        "CONFIG_PATH = HOME_PATH + \"config/\"\n",
        "TRANSACTIONS_PATH = HOME_PATH + \"transactions/\"\n",
        "ORDERS_PATH = HOME_PATH + \"orders/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the filenames along with their respective prefixes in a dictionary\n",
        "file_dict = {\n",
        "    \"header_files\": [\n",
        "        {\"filename\": \"amazon_order_headers_axel.csv\", \"origin\": \"A\"},\n",
        "        {\"filename\": \"amazon_order_headers_danielle.csv\", \"origin\": \"D\"}\n",
        "    ],\n",
        "    \"item_files\": [\n",
        "        {\"filename\": \"amazon_order_items_axel.csv\", \"origin\": \"A\"},\n",
        "        {\"filename\": \"amazon_order_items_danielle.csv\", \"origin\": \"D\"}\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "Cl2oedNw-QEr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Header and Item files"
      ],
      "metadata": {
        "id": "B0eMAjB-TKqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonOrderFilesProcessor:\n",
        "    def __init__(self, base_path, file_dict):\n",
        "        self.base_path = base_path\n",
        "        self.file_dict = file_dict\n",
        "        self.combined_header_df = None\n",
        "        self.combined_item_df = None\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_date_format(date_str):\n",
        "        if not isinstance(date_str, str):  # Add a check for non-string values\n",
        "            print(f\"Warning: Encountered non-string date value: {date_str}\")\n",
        "            return date_str\n",
        "\n",
        "        pattern = r'(\\d{1,2})\\s*(\\d{1,2})\\s*(\\d{4})'\n",
        "        match = re.match(pattern, date_str)\n",
        "        if match:\n",
        "            month, day, year = match.groups()\n",
        "            return f\"{year}-{int(month):02d}-{int(day):02d}\"\n",
        "        else:\n",
        "            return date_str\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_multiple_payments(payment_string):\n",
        "        if not isinstance(payment_string, str):\n",
        "            return []\n",
        "\n",
        "        pattern = r\"(?:MasterCard|Visa)?(?:\\s*ending\\s*in\\s*\\d{4})?:?\\s*([A-Za-z]*\\s*\\d{1,2},?\\s*\\d{4}|\\d{4}-\\d{1,2}-\\d{1,2})[^$]*\\$\\s*([\\d,]+\\.\\d{2})\"\n",
        "        matches = re.findall(pattern, payment_string)\n",
        "\n",
        "        month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "        month_map = {month: str(index + 1) for index, month in enumerate(month_names)}\n",
        "\n",
        "        processed_matches = []\n",
        "        for date, amount in matches:\n",
        "            for month, month_num in month_map.items():\n",
        "                date = date.replace(month, month_num)\n",
        "            date = date.replace(\",\", \"\").replace(\" \", \"-\")\n",
        "            date = AmazonOrderFilesProcessor.normalize_date_format(date)  # normalize the date format here\n",
        "            processed_matches.append((date, float(amount.replace(',', ''))))\n",
        "\n",
        "        return processed_matches\n",
        "\n",
        "    def split_multiple_payments(self, df):\n",
        "        new_rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            payments = self.extract_multiple_payments(row['Payments'])\n",
        "\n",
        "            for date, amount in payments:\n",
        "                new_row = row.copy()\n",
        "                new_row['Payment Date'] = pd.to_datetime(date)\n",
        "                new_row['Payment Amount'] = amount\n",
        "                new_rows.append(new_row)\n",
        "\n",
        "        return pd.DataFrame(new_rows)\n",
        "\n",
        "    def process_header_file(self, file_info):\n",
        "        df = pd.read_csv(self.base_path + file_info[\"filename\"])\n",
        "\n",
        "        # Standardize column spelling and drop unnecessary columns\n",
        "        df.columns = df.columns.str.capitalize()\n",
        "        df = df.rename(columns={'Order id': 'Order ID', 'Shipping_refund': 'Shipping Refund', 'Date': 'Order Date'})\n",
        "        df = df.drop(columns=['Items', 'To'])\n",
        "\n",
        "        # Remove header rows\n",
        "        df = df[(df['Order Date'] != 'date') & (df['Payments'] != 'payments')]\n",
        "\n",
        "        # Remove pending rows\n",
        "        df = df[(df['Total'] != 'pending')]\n",
        "\n",
        "        # Normalize the 'Date' column and turn it into a Date object\n",
        "        df['Order Date'] = df['Order Date'].apply(self.normalize_date_format)\n",
        "        df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "        # Add Origin and ID\n",
        "        df[\"Origin\"] = file_info[\"origin\"]\n",
        "        df[\"ID\"] = range(1, len(df) + 1)\n",
        "\n",
        "        return self.split_multiple_payments(df)\n",
        "\n",
        "    def process_item_file(self, file_info):\n",
        "        df = pd.read_csv(self.base_path + file_info[\"filename\"])\n",
        "\n",
        "        # Standardize column spelling\n",
        "        df.columns = df.columns.str.capitalize()\n",
        "        df = df.rename(columns={'Order id': 'Order ID', 'Order date': 'Order Date'})\n",
        "\n",
        "        # Remove header rows\n",
        "        df = df[(df['Price'] != 'price')]\n",
        "\n",
        "        # Normalize the 'Date' column and turn it into a Date object\n",
        "        df['Order Date'] = df['Order Date'].apply(self.normalize_date_format)\n",
        "        df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "        # Check for empty or NaN values in the 'Quantity' column and update accordingly\n",
        "        is_empty_quantity = df['Quantity'].isnull() | (df['Quantity'] == '')\n",
        "        df.loc[is_empty_quantity, 'Quantity'] = 1\n",
        "        df['chkQuantity'] = 'N'\n",
        "        df.loc[is_empty_quantity, 'chkQuantity'] = 'Y'\n",
        "\n",
        "        # Remove $ sign and format as float\n",
        "        df['Price'] = df['Price'].str.replace('$', '', regex=True).astype(float)\n",
        "\n",
        "        # Add Origin and ID\n",
        "        df[\"Origin\"] = file_info[\"origin\"]\n",
        "        df[\"ID\"] = range(1, len(df) + 1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def process_files(self):\n",
        "        header_dfs = [self.process_header_file(file_info) for file_info in self.file_dict[\"header_files\"]]\n",
        "        self.combined_header_df = pd.concat(header_dfs, ignore_index=True)\n",
        "\n",
        "        # Rearrange columns for header df\n",
        "        cols = ['Origin', 'ID'] + [col for col in self.combined_header_df if col not in ['Origin', 'ID']]\n",
        "        self.combined_header_df = self.combined_header_df[cols]\n",
        "\n",
        "        item_dfs = [self.process_item_file(file_info) for file_info in self.file_dict[\"item_files\"]]\n",
        "        self.combined_item_df = pd.concat(item_dfs, ignore_index=True)\n",
        "\n",
        "        # Rearrange columns for item df\n",
        "        cols = ['Origin', 'ID'] + [col for col in self.combined_item_df if col not in ['Origin', 'ID']]\n",
        "        self.combined_item_df = self.combined_item_df[cols]\n",
        "\n",
        "        return self.combined_header_df, self.combined_item_df\n",
        "\n",
        "# Create an instance of the class and process the files\n",
        "processor = AmazonOrderFilesProcessor(ORDERS_PATH, file_dict)\n",
        "combined_headers_df, combined_items_df = processor.process_files()"
      ],
      "metadata": {
        "id": "-M1r5d30Au8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f62106-072a-4317-c605-fc8f5ffd64d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Encountered non-string date value: nan\n",
            "Warning: Encountered non-string date value: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Combined Files"
      ],
      "metadata": {
        "id": "UuzSmhK_Y2IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export both files into Google Drive\n",
        "combined_headers_df.to_csv(f\"{ORDERS_PATH}amazon_headers_combined.csv\", index=False)\n",
        "combined_items_df.to_csv(f\"{ORDERS_PATH}amazon_items_combined.csv\", index=False)"
      ],
      "metadata": {
        "id": "LjmSgB-na5JN"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}